{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Huggingface Sagemaker-sdk - Getting Started Demo\n",
    "### Name Entity Recognition with `wnut_17` dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Introduction](#Introduction)  \n",
    "2. [Development Environment and Permissions](#Development-Environment-and-Permissions)\n",
    "    1. [Installation](#Installation)  \n",
    "    2. [Development environment](#Development-environment)  \n",
    "    3. [Permissions](#Permissions)\n",
    "3. [Processing](#Preprocessing)   \n",
    "    1. [Tokenization](#Tokenization)  \n",
    "    2. [Uploading data to sagemaker_session_bucket](#Uploading-data-to-sagemaker_session_bucket)  \n",
    "4. [Fine-tuning & starting Sagemaker Training Job](#Fine-tuning-\\&-starting-Sagemaker-Training-Job)  \n",
    "    1. [Creating an Estimator and start a training job](#Creating-an-Estimator-and-start-a-training-job)  \n",
    "    2. [Estimator Parameters](#Estimator-Parameters)   \n",
    "    3. [Download fine-tuned model from s3](#Download-fine-tuned-model-from-s3)\n",
    "    3. [Attach to old training job to an estimator ](#Attach-to-old-training-job-to-an-estimator)  \n",
    "5. [_Coming soon_:Push model to the Hugging Face hub](#Push-model-to-the-Hugging-Face-hub)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Welcome to our NER example. In this demo, we will use the Hugging Faces `transformers` and `datasets` library together with a custom Amazon sagemaker-sdk extension to fine-tune a pre-trained transformer for Name Entiity Recognition Usecase. In particular, the pre-trained model will be fine-tuned using the `wnut_17` dataset. To get started, we need to set up the environment with a few prerequisite steps, for permissions, configurations, and so on. \n",
    "\n",
    "[LINK TO WNUT_17 Dataset](https://huggingface.co/datasets/wnut_17)\n",
    "\n",
    "_**NOTE: You can run this demo in Sagemaker Studio, your local machine or Sagemaker Notebook Instances**_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Development Environment and Permissions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "_*Note:* we only install the required libraries from Hugging Face and AWS. You also need PyTorch or Tensorflow, if you havenÂ´t it installed_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sagemaker>=2.140.0 in /opt/conda/lib/python3.9/site-packages (2.179.0)\n",
      "Requirement already satisfied: transformers==4.26.1 in /opt/conda/lib/python3.9/site-packages (4.26.1)\n",
      "Requirement already satisfied: datasets[s3]==2.10.1 in /opt/conda/lib/python3.9/site-packages (2.10.1)\n",
      "Requirement already satisfied: evaluate in /opt/conda/lib/python3.9/site-packages (0.4.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.9/site-packages (from transformers==4.26.1) (0.13.3)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.9/site-packages (from transformers==4.26.1) (3.12.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /opt/conda/lib/python3.9/site-packages (from transformers==4.26.1) (0.16.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.9/site-packages (from transformers==4.26.1) (2023.8.8)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.9/site-packages (from transformers==4.26.1) (6.0.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.9/site-packages (from transformers==4.26.1) (1.23.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.9/site-packages (from transformers==4.26.1) (4.64.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.9/site-packages (from transformers==4.26.1) (23.0)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.9/site-packages (from transformers==4.26.1) (2.28.2)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.9/site-packages (from datasets[s3]==2.10.1) (1.5.3)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.9/site-packages (from datasets[s3]==2.10.1) (0.70.14)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /opt/conda/lib/python3.9/site-packages (from datasets[s3]==2.10.1) (2023.1.0)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.9/site-packages (from datasets[s3]==2.10.1) (3.8.5)\n",
      "Requirement already satisfied: responses<0.19 in /opt/conda/lib/python3.9/site-packages (from datasets[s3]==2.10.1) (0.18.0)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /opt/conda/lib/python3.9/site-packages (from datasets[s3]==2.10.1) (0.3.6)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.9/site-packages (from datasets[s3]==2.10.1) (3.3.0)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /opt/conda/lib/python3.9/site-packages (from datasets[s3]==2.10.1) (11.0.0)\n",
      "Requirement already satisfied: s3fs in /opt/conda/lib/python3.9/site-packages (from datasets[s3]==2.10.1) (0.4.2)\n",
      "Requirement already satisfied: pathos in /opt/conda/lib/python3.9/site-packages (from sagemaker>=2.140.0) (0.3.0)\n",
      "Requirement already satisfied: cloudpickle==2.2.1 in /opt/conda/lib/python3.9/site-packages (from sagemaker>=2.140.0) (2.2.1)\n",
      "Requirement already satisfied: google-pasta in /opt/conda/lib/python3.9/site-packages (from sagemaker>=2.140.0) (0.2.0)\n",
      "Requirement already satisfied: attrs<24,>=23.1.0 in /opt/conda/lib/python3.9/site-packages (from sagemaker>=2.140.0) (23.1.0)\n",
      "Requirement already satisfied: boto3<2.0,>=1.26.131 in /opt/conda/lib/python3.9/site-packages (from sagemaker>=2.140.0) (1.28.31)\n",
      "Requirement already satisfied: tblib==1.7.0 in /opt/conda/lib/python3.9/site-packages (from sagemaker>=2.140.0) (1.7.0)\n",
      "Requirement already satisfied: jsonschema in /opt/conda/lib/python3.9/site-packages (from sagemaker>=2.140.0) (4.19.0)\n",
      "Requirement already satisfied: platformdirs in /opt/conda/lib/python3.9/site-packages (from sagemaker>=2.140.0) (3.10.0)\n",
      "Requirement already satisfied: schema in /opt/conda/lib/python3.9/site-packages (from sagemaker>=2.140.0) (0.7.5)\n",
      "Requirement already satisfied: importlib-metadata<7.0,>=1.4.0 in /opt/conda/lib/python3.9/site-packages (from sagemaker>=2.140.0) (4.13.0)\n",
      "Requirement already satisfied: smdebug-rulesconfig==1.0.1 in /opt/conda/lib/python3.9/site-packages (from sagemaker>=2.140.0) (1.0.1)\n",
      "Requirement already satisfied: protobuf<5.0,>=3.12 in /opt/conda/lib/python3.9/site-packages (from sagemaker>=2.140.0) (3.20.2)\n",
      "Requirement already satisfied: botocore<1.32.0,>=1.31.31 in /opt/conda/lib/python3.9/site-packages (from boto3<2.0,>=1.26.131->sagemaker>=2.140.0) (1.31.31)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.9/site-packages (from boto3<2.0,>=1.26.131->sagemaker>=2.140.0) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /opt/conda/lib/python3.9/site-packages (from boto3<2.0,>=1.26.131->sagemaker>=2.140.0) (0.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets[s3]==2.10.1) (6.0.4)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets[s3]==2.10.1) (2.1.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets[s3]==2.10.1) (1.9.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets[s3]==2.10.1) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets[s3]==2.10.1) (4.0.3)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets[s3]==2.10.1) (1.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.26.1) (4.4.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.9/site-packages (from importlib-metadata<7.0,>=1.4.0->sagemaker>=2.140.0) (3.13.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests->transformers==4.26.1) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests->transformers==4.26.1) (1.26.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests->transformers==4.26.1) (3.4)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.9/site-packages (from google-pasta->sagemaker>=2.140.0) (1.16.0)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/conda/lib/python3.9/site-packages (from jsonschema->sagemaker>=2.140.0) (0.9.2)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/lib/python3.9/site-packages (from jsonschema->sagemaker>=2.140.0) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /opt/conda/lib/python3.9/site-packages (from jsonschema->sagemaker>=2.140.0) (0.30.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.9/site-packages (from pandas->datasets[s3]==2.10.1) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.9/site-packages (from pandas->datasets[s3]==2.10.1) (2022.7.1)\n",
      "Requirement already satisfied: pox>=0.3.2 in /opt/conda/lib/python3.9/site-packages (from pathos->sagemaker>=2.140.0) (0.3.2)\n",
      "Requirement already satisfied: ppft>=1.7.6.6 in /opt/conda/lib/python3.9/site-packages (from pathos->sagemaker>=2.140.0) (1.7.6.6)\n",
      "Requirement already satisfied: contextlib2>=0.5.5 in /opt/conda/lib/python3.9/site-packages (from schema->sagemaker>=2.140.0) (21.6.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install \"sagemaker>=2.140.0\" \"transformers==4.26.1\" \"datasets[s3]==2.10.1\" \"evaluate\" --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting jupyter_contrib_nbextensions\n",
      "  Using cached jupyter_contrib_nbextensions-0.7.0.tar.gz (23.5 MB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting IProgress\n",
      "  Using cached IProgress-0.4-py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: ipython_genutils in /opt/conda/lib/python3.9/site-packages (from jupyter_contrib_nbextensions) (0.2.0)\n",
      "Collecting jupyter_contrib_core>=0.3.3\n",
      "  Using cached jupyter_contrib_core-0.4.2.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: jupyter_core in /opt/conda/lib/python3.9/site-packages (from jupyter_contrib_nbextensions) (4.9.2)\n",
      "Collecting jupyter_highlight_selected_word>=0.1.1\n",
      "  Using cached jupyter_highlight_selected_word-0.2.0-py2.py3-none-any.whl (11 kB)\n",
      "Collecting jupyter_nbextensions_configurator>=0.4.0\n",
      "  Using cached jupyter_nbextensions_configurator-0.6.3-py2.py3-none-any.whl (466 kB)\n",
      "Collecting nbconvert>=6.0\n",
      "  Downloading nbconvert-7.7.4-py3-none-any.whl (254 kB)\n",
      "\u001b[2K     \u001b[90mâââââââââââââââââââââââââââââââââââââââ\u001b[0m \u001b[32m254.6/254.6 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting notebook>=6.0\n",
      "  Downloading notebook-7.0.2-py3-none-any.whl (4.0 MB)\n",
      "\u001b[2K     \u001b[90mââââââââââââââââââââââââââââââââââââââââ\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tornado in /opt/conda/lib/python3.9/site-packages (from jupyter_contrib_nbextensions) (6.2)\n",
      "Requirement already satisfied: traitlets>=4.1 in /opt/conda/lib/python3.9/site-packages (from jupyter_contrib_nbextensions) (5.9.0)\n",
      "Collecting lxml\n",
      "  Downloading lxml-4.9.3-cp39-cp39-manylinux_2_28_x86_64.whl (8.0 MB)\n",
      "\u001b[2K     \u001b[90mââââââââââââââââââââââââââââââââââââââââ\u001b[0m \u001b[32m8.0/8.0 MB\u001b[0m \u001b[31m39.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: six in /opt/conda/lib/python3.9/site-packages (from IProgress) (1.16.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.9/site-packages (from jupyter_contrib_core>=0.3.3->jupyter_contrib_nbextensions) (65.6.3)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.9/site-packages (from jupyter_nbextensions_configurator>=0.4.0->jupyter_contrib_nbextensions) (6.0.1)\n",
      "Collecting pandocfilters>=1.4.1\n",
      "  Downloading pandocfilters-1.5.0-py2.py3-none-any.whl (8.7 kB)\n",
      "Requirement already satisfied: importlib-metadata>=3.6 in /opt/conda/lib/python3.9/site-packages (from nbconvert>=6.0->jupyter_contrib_nbextensions) (4.13.0)\n",
      "Requirement already satisfied: markupsafe>=2.0 in /opt/conda/lib/python3.9/site-packages (from nbconvert>=6.0->jupyter_contrib_nbextensions) (2.1.2)\n",
      "Collecting defusedxml\n",
      "  Downloading defusedxml-0.7.1-py2.py3-none-any.whl (25 kB)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.9/site-packages (from nbconvert>=6.0->jupyter_contrib_nbextensions) (23.0)\n",
      "Requirement already satisfied: pygments>=2.4.1 in /opt/conda/lib/python3.9/site-packages (from nbconvert>=6.0->jupyter_contrib_nbextensions) (2.14.0)\n",
      "Collecting tinycss2\n",
      "  Downloading tinycss2-1.2.1-py3-none-any.whl (21 kB)\n",
      "Collecting jupyterlab-pygments\n",
      "  Downloading jupyterlab_pygments-0.2.2-py2.py3-none-any.whl (21 kB)\n",
      "Collecting mistune<4,>=2.0.3\n",
      "  Downloading mistune-3.0.1-py3-none-any.whl (47 kB)\n",
      "\u001b[2K     \u001b[90mââââââââââââââââââââââââââââââââââââââââ\u001b[0m \u001b[32m48.0/48.0 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting beautifulsoup4\n",
      "  Downloading beautifulsoup4-4.12.2-py3-none-any.whl (142 kB)\n",
      "\u001b[2K     \u001b[90mâââââââââââââââââââââââââââââââââââââââ\u001b[0m \u001b[32m143.0/143.0 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting nbformat>=5.7\n",
      "  Downloading nbformat-5.9.2-py3-none-any.whl (77 kB)\n",
      "\u001b[2K     \u001b[90mââââââââââââââââââââââââââââââââââââââââ\u001b[0m \u001b[32m77.6/77.6 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting bleach!=5.0.0\n",
      "  Downloading bleach-6.0.0-py3-none-any.whl (162 kB)\n",
      "\u001b[2K     \u001b[90mâââââââââââââââââââââââââââââââââââââââ\u001b[0m \u001b[32m162.5/162.5 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting nbclient>=0.5.0\n",
      "  Downloading nbclient-0.8.0-py3-none-any.whl (73 kB)\n",
      "\u001b[2K     \u001b[90mââââââââââââââââââââââââââââââââââââââââ\u001b[0m \u001b[32m73.1/73.1 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: jinja2>=3.0 in /opt/conda/lib/python3.9/site-packages (from nbconvert>=6.0->jupyter_contrib_nbextensions) (3.1.2)\n",
      "Collecting jupyter-server<3,>=2.4.0\n",
      "  Downloading jupyter_server-2.7.2-py3-none-any.whl (375 kB)\n",
      "\u001b[2K     \u001b[90mâââââââââââââââââââââââââââââââââââââââ\u001b[0m \u001b[32m375.3/375.3 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting jupyterlab-server<3,>=2.22.1\n",
      "  Downloading jupyterlab_server-2.24.0-py3-none-any.whl (57 kB)\n",
      "\u001b[2K     \u001b[90mââââââââââââââââââââââââââââââââââââââââ\u001b[0m \u001b[32m57.3/57.3 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting jupyterlab<5,>=4.0.2\n",
      "  Downloading jupyterlab-4.0.5-py3-none-any.whl (9.2 MB)\n",
      "\u001b[2K     \u001b[90mââââââââââââââââââââââââââââââââââââââââ\u001b[0m \u001b[32m9.2/9.2 MB\u001b[0m \u001b[31m37.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting notebook-shim<0.3,>=0.2\n",
      "  Downloading notebook_shim-0.2.3-py3-none-any.whl (13 kB)\n",
      "Collecting webencodings\n",
      "  Downloading webencodings-0.5.1-py2.py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.9/site-packages (from importlib-metadata>=3.6->nbconvert>=6.0->jupyter_contrib_nbextensions) (3.13.0)\n",
      "Collecting jupyter-server-terminals\n",
      "  Downloading jupyter_server_terminals-0.4.4-py3-none-any.whl (13 kB)\n",
      "Collecting prometheus-client\n",
      "  Downloading prometheus_client-0.17.1-py3-none-any.whl (60 kB)\n",
      "\u001b[2K     \u001b[90mââââââââââââââââââââââââââââââââââââââââ\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting send2trash>=1.8.2\n",
      "  Downloading Send2Trash-1.8.2-py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: pyzmq>=24 in /opt/conda/lib/python3.9/site-packages (from jupyter-server<3,>=2.4.0->notebook>=6.0->jupyter_contrib_nbextensions) (24.0.1)\n",
      "Collecting overrides\n",
      "  Downloading overrides-7.4.0-py3-none-any.whl (17 kB)\n",
      "Collecting anyio>=3.1.0\n",
      "  Downloading anyio-3.7.1-py3-none-any.whl (80 kB)\n",
      "\u001b[2K     \u001b[90mââââââââââââââââââââââââââââââââââââââââ\u001b[0m \u001b[32m80.9/80.9 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting websocket-client\n",
      "  Downloading websocket_client-1.6.1-py3-none-any.whl (56 kB)\n",
      "\u001b[2K     \u001b[90mâââââââââââââââââââââââââââââââââââââââ\u001b[0m \u001b[32m56.9/56.9 kB\u001b[0m \u001b[31m987.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting jupyter-client>=7.4.4\n",
      "  Downloading jupyter_client-8.3.0-py3-none-any.whl (103 kB)\n",
      "\u001b[2K     \u001b[90mâââââââââââââââââââââââââââââââââââââââ\u001b[0m \u001b[32m103.2/103.2 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting terminado>=0.8.3\n",
      "  Downloading terminado-0.17.1-py3-none-any.whl (17 kB)\n",
      "Collecting jupyter_core\n",
      "  Downloading jupyter_core-5.3.1-py3-none-any.whl (93 kB)\n",
      "\u001b[2K     \u001b[90mââââââââââââââââââââââââââââââââââââââââ\u001b[0m \u001b[32m93.7/93.7 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hCollecting argon2-cffi\n",
      "  Downloading argon2_cffi-23.1.0-py3-none-any.whl (15 kB)\n",
      "Collecting jupyter-events>=0.6.0\n",
      "  Downloading jupyter_events-0.7.0-py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /opt/conda/lib/python3.9/site-packages (from jupyter_core->jupyter_contrib_nbextensions) (3.10.0)\n",
      "Collecting async-lru>=1.0.0\n",
      "  Downloading async_lru-2.0.4-py3-none-any.whl (6.1 kB)\n",
      "Collecting jupyter-lsp>=2.0.0\n",
      "  Downloading jupyter_lsp-2.2.0-py3-none-any.whl (65 kB)\n",
      "\u001b[2K     \u001b[90mââââââââââââââââââââââââââââââââââââââââ\u001b[0m \u001b[32m66.0/66.0 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: ipykernel in /opt/conda/lib/python3.9/site-packages (from jupyterlab<5,>=4.0.2->notebook>=6.0->jupyter_contrib_nbextensions) (5.5.6)\n",
      "Collecting tomli\n",
      "  Downloading tomli-2.0.1-py3-none-any.whl (12 kB)\n",
      "Collecting babel>=2.10\n",
      "  Downloading Babel-2.12.1-py3-none-any.whl (10.1 MB)\n",
      "\u001b[2K     \u001b[90mââââââââââââââââââââââââââââââââââââââââ\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m46.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests>=2.28 in /opt/conda/lib/python3.9/site-packages (from jupyterlab-server<3,>=2.22.1->notebook>=6.0->jupyter_contrib_nbextensions) (2.28.2)\n",
      "Requirement already satisfied: jsonschema>=4.17.3 in /opt/conda/lib/python3.9/site-packages (from jupyterlab-server<3,>=2.22.1->notebook>=6.0->jupyter_contrib_nbextensions) (4.19.0)\n",
      "Collecting json5>=0.9.0\n",
      "  Downloading json5-0.9.14-py2.py3-none-any.whl (19 kB)\n",
      "Collecting fastjsonschema\n",
      "  Downloading fastjsonschema-2.18.0-py3-none-any.whl (23 kB)\n",
      "Collecting soupsieve>1.2\n",
      "  Downloading soupsieve-2.4.1-py3-none-any.whl (36 kB)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/conda/lib/python3.9/site-packages (from anyio>=3.1.0->jupyter-server<3,>=2.4.0->notebook>=6.0->jupyter_contrib_nbextensions) (3.4)\n",
      "Collecting sniffio>=1.1\n",
      "  Downloading sniffio-1.3.0-py3-none-any.whl (10 kB)\n",
      "Collecting exceptiongroup\n",
      "  Downloading exceptiongroup-1.1.3-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /opt/conda/lib/python3.9/site-packages (from async-lru>=1.0.0->jupyterlab<5,>=4.0.2->notebook>=6.0->jupyter_contrib_nbextensions) (4.4.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/lib/python3.9/site-packages (from jsonschema>=4.17.3->jupyterlab-server<3,>=2.22.1->notebook>=6.0->jupyter_contrib_nbextensions) (2023.7.1)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /opt/conda/lib/python3.9/site-packages (from jsonschema>=4.17.3->jupyterlab-server<3,>=2.22.1->notebook>=6.0->jupyter_contrib_nbextensions) (23.1.0)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/conda/lib/python3.9/site-packages (from jsonschema>=4.17.3->jupyterlab-server<3,>=2.22.1->notebook>=6.0->jupyter_contrib_nbextensions) (0.9.2)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /opt/conda/lib/python3.9/site-packages (from jsonschema>=4.17.3->jupyterlab-server<3,>=2.22.1->notebook>=6.0->jupyter_contrib_nbextensions) (0.30.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.9/site-packages (from jupyter-client>=7.4.4->jupyter-server<3,>=2.4.0->notebook>=6.0->jupyter_contrib_nbextensions) (2.8.2)\n",
      "Collecting python-json-logger>=2.0.4\n",
      "  Downloading python_json_logger-2.0.7-py3-none-any.whl (8.1 kB)\n",
      "Collecting rfc3339-validator\n",
      "  Downloading rfc3339_validator-0.1.4-py2.py3-none-any.whl (3.5 kB)\n",
      "Collecting rfc3986-validator>=0.1.1\n",
      "  Downloading rfc3986_validator-0.1.1-py2.py3-none-any.whl (4.2 kB)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests>=2.28->jupyterlab-server<3,>=2.22.1->notebook>=6.0->jupyter_contrib_nbextensions) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests>=2.28->jupyterlab-server<3,>=2.22.1->notebook>=6.0->jupyter_contrib_nbextensions) (1.26.14)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.9/site-packages (from requests>=2.28->jupyterlab-server<3,>=2.22.1->notebook>=6.0->jupyter_contrib_nbextensions) (2.1.1)\n",
      "Requirement already satisfied: ptyprocess in /opt/conda/lib/python3.9/site-packages (from terminado>=0.8.3->jupyter-server<3,>=2.4.0->notebook>=6.0->jupyter_contrib_nbextensions) (0.7.0)\n",
      "Collecting argon2-cffi-bindings\n",
      "  Downloading argon2_cffi_bindings-21.2.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (86 kB)\n",
      "\u001b[2K     \u001b[90mââââââââââââââââââââââââââââââââââââââââ\u001b[0m \u001b[32m86.2/86.2 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: ipython>=5.0.0 in /opt/conda/lib/python3.9/site-packages (from ipykernel->jupyterlab<5,>=4.0.2->notebook>=6.0->jupyter_contrib_nbextensions) (8.10.0)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.9/site-packages (from ipython>=5.0.0->ipykernel->jupyterlab<5,>=4.0.2->notebook>=6.0->jupyter_contrib_nbextensions) (0.18.2)\n",
      "Requirement already satisfied: matplotlib-inline in /opt/conda/lib/python3.9/site-packages (from ipython>=5.0.0->ipykernel->jupyterlab<5,>=4.0.2->notebook>=6.0->jupyter_contrib_nbextensions) (0.1.6)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.9/site-packages (from ipython>=5.0.0->ipykernel->jupyterlab<5,>=4.0.2->notebook>=6.0->jupyter_contrib_nbextensions) (4.8.0)\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.9/site-packages (from ipython>=5.0.0->ipykernel->jupyterlab<5,>=4.0.2->notebook>=6.0->jupyter_contrib_nbextensions) (5.1.1)\n",
      "Requirement already satisfied: pickleshare in /opt/conda/lib/python3.9/site-packages (from ipython>=5.0.0->ipykernel->jupyterlab<5,>=4.0.2->notebook>=6.0->jupyter_contrib_nbextensions) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.30 in /opt/conda/lib/python3.9/site-packages (from ipython>=5.0.0->ipykernel->jupyterlab<5,>=4.0.2->notebook>=6.0->jupyter_contrib_nbextensions) (3.0.36)\n",
      "Requirement already satisfied: stack-data in /opt/conda/lib/python3.9/site-packages (from ipython>=5.0.0->ipykernel->jupyterlab<5,>=4.0.2->notebook>=6.0->jupyter_contrib_nbextensions) (0.6.2)\n",
      "Requirement already satisfied: backcall in /opt/conda/lib/python3.9/site-packages (from ipython>=5.0.0->ipykernel->jupyterlab<5,>=4.0.2->notebook>=6.0->jupyter_contrib_nbextensions) (0.2.0)\n",
      "Collecting jsonpointer>1.13\n",
      "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
      "Collecting fqdn\n",
      "  Downloading fqdn-1.5.1-py3-none-any.whl (9.1 kB)\n",
      "Collecting webcolors>=1.11\n",
      "  Downloading webcolors-1.13-py3-none-any.whl (14 kB)\n",
      "Collecting isoduration\n",
      "  Downloading isoduration-20.11.0-py3-none-any.whl (11 kB)\n",
      "Collecting uri-template\n",
      "  Downloading uri_template-1.3.0-py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: cffi>=1.0.1 in /opt/conda/lib/python3.9/site-packages (from argon2-cffi-bindings->argon2-cffi->jupyter-server<3,>=2.4.0->notebook>=6.0->jupyter_contrib_nbextensions) (1.15.1)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.9/site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->jupyter-server<3,>=2.4.0->notebook>=6.0->jupyter_contrib_nbextensions) (2.21)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /opt/conda/lib/python3.9/site-packages (from jedi>=0.16->ipython>=5.0.0->ipykernel->jupyterlab<5,>=4.0.2->notebook>=6.0->jupyter_contrib_nbextensions) (0.8.3)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.9/site-packages (from prompt-toolkit<3.1.0,>=3.0.30->ipython>=5.0.0->ipykernel->jupyterlab<5,>=4.0.2->notebook>=6.0->jupyter_contrib_nbextensions) (0.2.6)\n",
      "Collecting arrow>=0.15.0\n",
      "  Downloading arrow-1.2.3-py3-none-any.whl (66 kB)\n",
      "\u001b[2K     \u001b[90mââââââââââââââââââââââââââââââââââââââââ\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: asttokens>=2.1.0 in /opt/conda/lib/python3.9/site-packages (from stack-data->ipython>=5.0.0->ipykernel->jupyterlab<5,>=4.0.2->notebook>=6.0->jupyter_contrib_nbextensions) (2.2.1)\n",
      "Requirement already satisfied: pure-eval in /opt/conda/lib/python3.9/site-packages (from stack-data->ipython>=5.0.0->ipykernel->jupyterlab<5,>=4.0.2->notebook>=6.0->jupyter_contrib_nbextensions) (0.2.2)\n",
      "Requirement already satisfied: executing>=1.2.0 in /opt/conda/lib/python3.9/site-packages (from stack-data->ipython>=5.0.0->ipykernel->jupyterlab<5,>=4.0.2->notebook>=6.0->jupyter_contrib_nbextensions) (1.2.0)\n",
      "Building wheels for collected packages: jupyter_contrib_nbextensions, jupyter_contrib_core\n",
      "  Building wheel for jupyter_contrib_nbextensions (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for jupyter_contrib_nbextensions: filename=jupyter_contrib_nbextensions-0.7.0-py2.py3-none-any.whl size=23428787 sha256=cd45c7cc31b4c251b790f85911e084d6316fb853e986933cdb53bdb07f116b34\n",
      "  Stored in directory: /root/.cache/pip/wheels/e7/99/91/7f24a075786a6760f3ac32ab5fa92e1d1a90e0d2cd8958adfd\n",
      "  Building wheel for jupyter_contrib_core (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for jupyter_contrib_core: filename=jupyter_contrib_core-0.4.2-py2.py3-none-any.whl size=17490 sha256=a5c123ee8b993de9fc80fa198ac737dd31e06197430f7dbaeeda064cf5bc75cd\n",
      "  Stored in directory: /root/.cache/pip/wheels/57/9f/80/32c07b8a950a45f6cf8cd5980c22a27ce514c27b795250e497\n",
      "Successfully built jupyter_contrib_nbextensions jupyter_contrib_core\n",
      "Installing collected packages: webencodings, jupyter_highlight_selected_word, json5, fastjsonschema, websocket-client, webcolors, uri-template, tomli, tinycss2, terminado, soupsieve, sniffio, send2trash, rfc3986-validator, rfc3339-validator, python-json-logger, prometheus-client, pandocfilters, overrides, mistune, lxml, jupyterlab-pygments, jupyter_core, jsonpointer, IProgress, fqdn, exceptiongroup, defusedxml, bleach, babel, async-lru, jupyter-server-terminals, jupyter-client, beautifulsoup4, arrow, argon2-cffi-bindings, anyio, isoduration, argon2-cffi, nbformat, nbclient, jupyter-events, nbconvert, jupyter-server, notebook-shim, jupyterlab-server, jupyter-lsp, jupyterlab, notebook, jupyter_contrib_core, jupyter_nbextensions_configurator, jupyter_contrib_nbextensions\n",
      "  Attempting uninstall: jupyter_core\n",
      "    Found existing installation: jupyter-core 4.9.2\n",
      "    Uninstalling jupyter-core-4.9.2:\n",
      "      Successfully uninstalled jupyter-core-4.9.2\n",
      "  Attempting uninstall: jupyter-client\n",
      "    Found existing installation: jupyter-client 6.1.5\n",
      "    Uninstalling jupyter-client-6.1.5:\n",
      "      Successfully uninstalled jupyter-client-6.1.5\n",
      "Successfully installed IProgress-0.4 anyio-3.7.1 argon2-cffi-23.1.0 argon2-cffi-bindings-21.2.0 arrow-1.2.3 async-lru-2.0.4 babel-2.12.1 beautifulsoup4-4.12.2 bleach-6.0.0 defusedxml-0.7.1 exceptiongroup-1.1.3 fastjsonschema-2.18.0 fqdn-1.5.1 isoduration-20.11.0 json5-0.9.14 jsonpointer-2.4 jupyter-client-8.3.0 jupyter-events-0.7.0 jupyter-lsp-2.2.0 jupyter-server-2.7.2 jupyter-server-terminals-0.4.4 jupyter_contrib_core-0.4.2 jupyter_contrib_nbextensions-0.7.0 jupyter_core-5.3.1 jupyter_highlight_selected_word-0.2.0 jupyter_nbextensions_configurator-0.6.3 jupyterlab-4.0.5 jupyterlab-pygments-0.2.2 jupyterlab-server-2.24.0 lxml-4.9.3 mistune-3.0.1 nbclient-0.8.0 nbconvert-7.7.4 nbformat-5.9.2 notebook-7.0.2 notebook-shim-0.2.3 overrides-7.4.0 pandocfilters-1.5.0 prometheus-client-0.17.1 python-json-logger-2.0.7 rfc3339-validator-0.1.4 rfc3986-validator-0.1.1 send2trash-1.8.2 sniffio-1.3.0 soupsieve-2.4.1 terminado-0.17.1 tinycss2-1.2.1 tomli-2.0.1 uri-template-1.3.0 webcolors-1.13 webencodings-0.5.1 websocket-client-1.6.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install jupyter_contrib_nbextensions IProgress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: jupyter [-h] [--version] [--config-dir] [--data-dir] [--runtime-dir]\n",
      "               [--paths] [--json] [--debug]\n",
      "               [subcommand]\n",
      "\n",
      "Jupyter: Interactive Computing\n",
      "\n",
      "positional arguments:\n",
      "  subcommand     the subcommand to launch\n",
      "\n",
      "optional arguments:\n",
      "  -h, --help     show this help message and exit\n",
      "  --version      show the versions of core jupyter packages and exit\n",
      "  --config-dir   show Jupyter config dir\n",
      "  --data-dir     show Jupyter data dir\n",
      "  --runtime-dir  show Jupyter runtime dir\n",
      "  --paths        show all Jupyter paths. Add --json for machine-readable\n",
      "                 format.\n",
      "  --json         output paths as machine-readable json\n",
      "  --debug        output debug information about paths\n",
      "\n",
      "Available subcommands: contrib dejavu events execute kernel kernelspec lab\n",
      "labextension labhub migrate nbconvert nbextensions_configurator notebook run\n",
      "server troubleshoot trust\n",
      "\n",
      "Jupyter command `jupyter-nbextension` not found.\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbextension enable --py widgetsnbextension\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Development environment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker.huggingface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Permissions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_If you are going to use Sagemaker in a local environment. You need access to an IAM Role with the required permissions for Sagemaker. You can find [here](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html) more about it._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker role arn: arn:aws:iam::151657023715:role/cfnstudiodomain-SageMakerExecutionRole-19WC4HFFELAEC\n",
      "sagemaker bucket: sagemaker-us-east-1-151657023715\n",
      "sagemaker session region: us-east-1\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "We are using the `datasets` library to download and preprocess the `imdb` dataset. After preprocessing, the dataset will be uploaded to our `sagemaker_session_bucket` to be used within our training job. The [imdb](http://ai.stanford.edu/~amaas/data/sentiment/) dataset consists of 25000 training and 25000 testing highly polar movie reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# tokenizer used in preprocessing\n",
    "tokenizer_name = 'distilbert-base-uncased'\n",
    "\n",
    "# dataset used\n",
    "dataset_name = 'wnut_17'\n",
    "\n",
    "# s3 key prefix for the data\n",
    "s3_prefix = 'samples/datasets/wnut_17'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset wnut_17 (/root/.cache/huggingface/datasets/wnut_17/wnut_17/1.0.0/077c7f08b8dbc800692e8c9186cdf3606d5849ab0e7be662e6135bb10eba54f9)\n",
      "100%|ââââââââââ| 3/3 [00:00<00:00, 86.82it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(dataset_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# download tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datasets from Huggingface are nice in two ways:\n",
    "- First, it allows you to really easily try a model for a task\n",
    "- gives you a good intuition how the dataset should be structured before going to deep\n",
    "- allows you to quickly judge how hard a certain task could be for a model.\n",
    "- you can explore those datasets very quickly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '0', 'tokens': ['@paulwalk', 'It', \"'s\", 'the', 'view', 'from', 'where', 'I', \"'m\", 'living', 'for', 'two', 'weeks', '.', 'Empire', 'State', 'Building', '=', 'ESB', '.', 'Pretty', 'bad', 'storm', 'here', 'last', 'evening', '.'], 'ner_tags': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 8, 8, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "print(dataset[\"train\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'B-corporation', 'I-corporation', 'B-creative-work', 'I-creative-work', 'B-group', 'I-group', 'B-location', 'I-location', 'B-person', 'I-person', 'B-product', 'I-product']\n"
     ]
    }
   ],
   "source": [
    "label_list = dataset[\"train\"].features[f\"ner_tags\"].feature.names\n",
    "print(label_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets look at how a tokenized example looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertTokenizerFast(name_or_path='distilbert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', '@', 'paul', '##walk', 'it', \"'\", 's', 'the', 'view', 'from', 'where', 'i', \"'\", 'm', 'living', 'for', 'two', 'weeks', '.', 'empire', 'state', 'building', '=', 'es', '##b', '.', 'pretty', 'bad', 'storm', 'here', 'last', 'evening', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "example = dataset[\"train\"][0]\n",
    "tokenized_input = tokenizer(example[\"tokens\"], is_split_into_words=True)\n",
    "tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying the tokenization to the whole datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[f\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)  # Map tokens to their respective word.\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:  # Set the special tokens to -100.\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:  # Only label the first token of a given word.\n",
    "                label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "To apply the preprocessing function over the entire dataset, use ð¤ Datasets map function. You can speed up the map function by setting batched=True to process multiple elements of the dataset at once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/wnut_17/wnut_17/1.0.0/077c7f08b8dbc800692e8c9186cdf3606d5849ab0e7be662e6135bb10eba54f9/cache-ecc04dc5f711955e.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/wnut_17/wnut_17/1.0.0/077c7f08b8dbc800692e8c9186cdf3606d5849ab0e7be662e6135bb10eba54f9/cache-b27c9135bb1bacbc.arrow\n"
     ]
    }
   ],
   "source": [
    "tokenized_wnut = dataset.map(tokenize_and_align_labels, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: evaluate in /opt/conda/lib/python3.9/site-packages (0.4.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.9/site-packages (from evaluate) (23.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.9/site-packages (from evaluate) (4.64.1)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.9/site-packages (from evaluate) (2023.1.0)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.9/site-packages (from evaluate) (0.70.14)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.9/site-packages (from evaluate) (1.5.3)\n",
      "Requirement already satisfied: dill in /opt/conda/lib/python3.9/site-packages (from evaluate) (0.3.6)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.9/site-packages (from evaluate) (2.10.1)\n",
      "Requirement already satisfied: responses<0.19 in /opt/conda/lib/python3.9/site-packages (from evaluate) (0.18.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.9/site-packages (from evaluate) (2.28.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.9/site-packages (from evaluate) (1.23.5)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.9/site-packages (from evaluate) (3.3.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.9/site-packages (from evaluate) (0.16.4)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.9/site-packages (from datasets>=2.0.0->evaluate) (3.8.5)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.9/site-packages (from datasets>=2.0.0->evaluate) (6.0.1)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /opt/conda/lib/python3.9/site-packages (from datasets>=2.0.0->evaluate) (11.0.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.9/site-packages (from huggingface-hub>=0.7.0->evaluate) (3.12.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.9/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->evaluate) (2.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->evaluate) (1.26.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->evaluate) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->evaluate) (2022.12.7)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.9/site-packages (from pandas->evaluate) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.9/site-packages (from pandas->evaluate) (2022.7.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.1.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas->evaluate) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "labels = [label_list[i] for i in example[f\"ner_tags\"]]\n",
    "\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = seqeval.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset = dataset[\"train\"]\n",
    "# train_dataset.set_format(\"csv\")\n",
    "validation_dataset = dataset[\"validation\"]\n",
    "# validation_dataset.set_format(\"csv\")\n",
    "test_dataset = dataset[\"test\"]\n",
    "# test_dataset.set_format(\"csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'tokens', 'ner_tags'],\n",
       "    num_rows: 3394\n",
       "})"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uploading data to `sagemaker_session_bucket`\n",
    "\n",
    "After we processed the `datasets` we are going to use the new `FileSystem` [integration](https://huggingface.co/docs/datasets/filesystems.html) to upload our dataset to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                              \r"
     ]
    }
   ],
   "source": [
    "# save train_dataset to s3\n",
    "training_input_path = f's3://{sess.default_bucket()}/{s3_prefix}/train'\n",
    "train_dataset.save_to_disk(training_input_path)\n",
    "\n",
    "# save validaiton_dataset to s3\n",
    "validation_input_path = f's3://{sess.default_bucket()}/{s3_prefix}/validation'\n",
    "validation_dataset.save_to_disk(validation_input_path)\n",
    "\n",
    "\n",
    "# save test_dataset to s3\n",
    "test_input_path = f's3://{sess.default_bucket()}/{s3_prefix}/test'\n",
    "test_dataset.save_to_disk(test_input_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning & starting Sagemaker Training Job\n",
    "\n",
    "In order to create a sagemaker training job we need an `HuggingFace` Estimator. The Estimator handles end-to-end Amazon SageMaker training and deployment tasks. In a Estimator we define, which fine-tuning script should be used as `entry_point`, which `instance_type` should be used, which `hyperparameters` are passed in .....\n",
    "\n",
    "\n",
    "\n",
    "```python\n",
    "huggingface_estimator = HuggingFace(entry_point='train.py',\n",
    "                            source_dir='./scripts',\n",
    "                            base_job_name='huggingface-sdk-extension',\n",
    "                            instance_type='ml.p3.2xlarge',\n",
    "                            instance_count=1,\n",
    "                            transformers_version='4.4',\n",
    "                            pytorch_version='1.6',\n",
    "                            py_version='py36',\n",
    "                            role=role,\n",
    "                            hyperparameters = {'epochs': 1,\n",
    "                                               'train_batch_size': 32,\n",
    "                                               'model_name':'distilbert-base-uncased'\n",
    "                                                })\n",
    "```\n",
    "\n",
    "When we create a SageMaker training job, SageMaker takes care of starting and managing all the required ec2 instances for us with the `huggingface` container, uploads the provided fine-tuning script `train.py` and downloads the data from our `sagemaker_session_bucket` into the container at `/opt/ml/input/data`. Then, it starts the training job by running. \n",
    "\n",
    "```python\n",
    "/opt/conda/bin/python train.py --epochs 1 --model_name distilbert-base-uncased --train_batch_size 32\n",
    "```\n",
    "\n",
    "The `hyperparameters` you define in the `HuggingFace` estimator are passed in as named arguments. \n",
    "\n",
    "Sagemaker is providing useful properties about the training environment through various environment variables, including the following:\n",
    "\n",
    "* `SM_MODEL_DIR`: A string that represents the path where the training job writes the model artifacts to. After training, artifacts in this directory are uploaded to S3 for model hosting.\n",
    "\n",
    "* `SM_NUM_GPUS`: An integer representing the number of GPUs available to the host.\n",
    "\n",
    "* `SM_CHANNEL_XXXX:` A string that represents the path to the directory that contains the input data for the specified channel. For example, if you specify two input channels in the HuggingFace estimatorâs fit call, named `train` and `test`, the environment variables `SM_CHANNEL_TRAIN` and `SM_CHANNEL_TEST` are set.\n",
    "\n",
    "\n",
    "To run your training job locally you can define `instance_type='local'` or `instance_type='local_gpu'` for gpu usage. _Note: this does not working within SageMaker Studio_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "from sagemaker.huggingface import HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\"test-trainer\",\n",
    "                                  per_device_train_batch_size=32,\n",
    "                                  per_device_eval_batch_size=32,\n",
    "                                  num_train_epochs=100,\n",
    "                                  learning_rate=2e-5,\n",
    "                                  weight_decay=0.01\n",
    "                                 )\n",
    "print(type(training_args.to_dict()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating an Estimator and start a training job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using provided s3_resource\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: huggingface-pytorch-training-2023-08-22-14-31-55-436\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-08-22 14:32:19 Starting - Starting the training job...\n",
      "2023-08-22 14:32:44 Starting - Preparing the instances for training.........\n",
      "2023-08-22 14:34:17 Downloading - Downloading input data\n",
      "2023-08-22 14:34:17 Training - Downloading the training image.....................\n",
      "2023-08-22 14:37:43 Training - Training image download completed. Training in progress...\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2023-08-22 14:38:01,676 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2023-08-22 14:38:01,700 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-08-22 14:38:01,715 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2023-08-22 14:38:01,718 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2023-08-22 14:38:02,000 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.9 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: accelerate>=0.12.0 in /opt/conda/lib/python3.9/site-packages (from -r requirements.txt (line 1)) (0.16.0)\u001b[0m\n",
      "\u001b[34mCollecting seqeval\u001b[0m\n",
      "\u001b[34mDownloading seqeval-1.2.2.tar.gz (43 kB)\u001b[0m\n",
      "\u001b[34mââââââââââââââââââââââââââââââââââââââââ 43.6/43.6 kB 3.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: datasets>=1.8.0 in /opt/conda/lib/python3.9/site-packages (from -r requirements.txt (line 3)) (2.9.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch>=1.3 in /opt/conda/lib/python3.9/site-packages (from -r requirements.txt (line 4)) (1.13.1+cu117)\u001b[0m\n",
      "\u001b[34mCollecting evaluate\u001b[0m\n",
      "\u001b[34mDownloading evaluate-0.4.0-py3-none-any.whl (81 kB)\u001b[0m\n",
      "\u001b[34mââââââââââââââââââââââââââââââââââââââââ 81.4/81.4 kB 12.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: psutil in /opt/conda/lib/python3.9/site-packages (from accelerate>=0.12.0->-r requirements.txt (line 1)) (5.9.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.9/site-packages (from accelerate>=0.12.0->-r requirements.txt (line 1)) (23.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml in /opt/conda/lib/python3.9/site-packages (from accelerate>=0.12.0->-r requirements.txt (line 1)) (5.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.9/site-packages (from accelerate>=0.12.0->-r requirements.txt (line 1)) (1.23.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: scikit-learn>=0.21.3 in /opt/conda/lib/python3.9/site-packages (from seqeval->-r requirements.txt (line 2)) (1.2.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dill<0.3.7 in /opt/conda/lib/python3.9/site-packages (from datasets>=1.8.0->-r requirements.txt (line 3)) (0.3.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec[http]>=2021.11.1 in /opt/conda/lib/python3.9/site-packages (from datasets>=1.8.0->-r requirements.txt (line 3)) (2023.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.9/site-packages (from datasets>=1.8.0->-r requirements.txt (line 3)) (2.28.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.9/site-packages (from datasets>=1.8.0->-r requirements.txt (line 3)) (0.18.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.9/site-packages (from datasets>=1.8.0->-r requirements.txt (line 3)) (1.5.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow>=6.0.0 in /opt/conda/lib/python3.9/site-packages (from datasets>=1.8.0->-r requirements.txt (line 3)) (11.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in /opt/conda/lib/python3.9/site-packages (from datasets>=1.8.0->-r requirements.txt (line 3)) (0.12.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: xxhash in /opt/conda/lib/python3.9/site-packages (from datasets>=1.8.0->-r requirements.txt (line 3)) (3.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiohttp in /opt/conda/lib/python3.9/site-packages (from datasets>=1.8.0->-r requirements.txt (line 3)) (3.8.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.9/site-packages (from datasets>=1.8.0->-r requirements.txt (line 3)) (0.70.14)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.9/site-packages (from datasets>=1.8.0->-r requirements.txt (line 3)) (4.64.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.9/site-packages (from torch>=1.3->-r requirements.txt (line 4)) (4.4.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 3)) (1.3.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 3)) (1.3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 3)) (6.0.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 3)) (2.1.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 3)) (4.0.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 3)) (22.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 3)) (1.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.9/site-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets>=1.8.0->-r requirements.txt (line 3)) (3.9.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->datasets>=1.8.0->-r requirements.txt (line 3)) (3.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->datasets>=1.8.0->-r requirements.txt (line 3)) (2022.12.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->datasets>=1.8.0->-r requirements.txt (line 3)) (1.26.14)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.9/site-packages (from scikit-learn>=0.21.3->seqeval->-r requirements.txt (line 2)) (1.10.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.9/site-packages (from scikit-learn>=0.21.3->seqeval->-r requirements.txt (line 2)) (3.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.9/site-packages (from scikit-learn>=0.21.3->seqeval->-r requirements.txt (line 2)) (1.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.9/site-packages (from pandas->datasets>=1.8.0->-r requirements.txt (line 3)) (2.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.9/site-packages (from pandas->datasets>=1.8.0->-r requirements.txt (line 3)) (2022.7.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas->datasets>=1.8.0->-r requirements.txt (line 3)) (1.16.0)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: seqeval\u001b[0m\n",
      "\u001b[34mBuilding wheel for seqeval (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for seqeval (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16164 sha256=9371c19c7afdc660d05321de35781dd20d13a2b4eaa7602b17d0940b0310034e\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/e2/a5/92/2c80d1928733611c2747a9820e1324a6835524d9411510c142\u001b[0m\n",
      "\u001b[34mSuccessfully built seqeval\u001b[0m\n",
      "\u001b[34mInstalling collected packages: seqeval, evaluate\u001b[0m\n",
      "\u001b[34mSuccessfully installed evaluate-0.4.0 seqeval-1.2.2\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip is available: 23.0 -> 23.2.1\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m2023-08-22 14:38:09,016 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2023-08-22 14:38:09,016 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2023-08-22 14:38:09,063 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-08-22 14:38:09,104 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-08-22 14:38:09,145 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-08-22 14:38:09,160 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {},\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.p3.2xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"dataset_name\": \"wnut_17\",\n",
      "        \"do_eval\": \"true\",\n",
      "        \"do_train\": \"true\",\n",
      "        \"model_name_or_path\": \"distilbert-base-uncased\",\n",
      "        \"num_train_epochs\": 1,\n",
      "        \"output_dir\": \"/opt/ml/model\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {},\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.p3.2xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": true,\n",
      "    \"job_name\": \"huggingface-pytorch-training-2023-08-22-14-31-55-436\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-151657023715/huggingface-pytorch-training-2023-08-22-14-31-55-436/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"run_ner\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 1,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.p3.2xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.p3.2xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"run_ner.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"dataset_name\":\"wnut_17\",\"do_eval\":\"true\",\"do_train\":\"true\",\"model_name_or_path\":\"distilbert-base-uncased\",\"num_train_epochs\":1,\"output_dir\":\"/opt/ml/model\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=run_ner.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p3.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.2xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.p3.2xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.2xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=run_ner\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-151657023715/huggingface-pytorch-training-2023-08-22-14-31-55-436/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.p3.2xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"dataset_name\":\"wnut_17\",\"do_eval\":\"true\",\"do_train\":\"true\",\"model_name_or_path\":\"distilbert-base-uncased\",\"num_train_epochs\":1,\"output_dir\":\"/opt/ml/model\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.2xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":true,\"job_name\":\"huggingface-pytorch-training-2023-08-22-14-31-55-436\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-151657023715/huggingface-pytorch-training-2023-08-22-14-31-55-436/source/sourcedir.tar.gz\",\"module_name\":\"run_ner\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":1,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p3.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.2xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"run_ner.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--dataset_name\",\"wnut_17\",\"--do_eval\",\"true\",\"--do_train\",\"true\",\"--model_name_or_path\",\"distilbert-base-uncased\",\"--num_train_epochs\",\"1\",\"--output_dir\",\"/opt/ml/model\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_HP_DATASET_NAME=wnut_17\u001b[0m\n",
      "\u001b[34mSM_HP_DO_EVAL=true\u001b[0m\n",
      "\u001b[34mSM_HP_DO_TRAIN=true\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_NAME_OR_PATH=distilbert-base-uncased\u001b[0m\n",
      "\u001b[34mSM_HP_NUM_TRAIN_EPOCHS=1\u001b[0m\n",
      "\u001b[34mSM_HP_OUTPUT_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python39.zip:/opt/conda/lib/python3.9:/opt/conda/lib/python3.9/lib-dynload:/opt/conda/lib/python3.9/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.9 run_ner.py --dataset_name wnut_17 --do_eval true --do_train true --model_name_or_path distilbert-base-uncased --num_train_epochs 1 --output_dir /opt/ml/model\u001b[0m\n",
      "\u001b[34m[2023-08-22 14:38:11.437: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.26.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m2023-08-22 14:38:11,445 root         INFO     Using NamedTuple = typing._NamedTuple instead.\u001b[0m\n",
      "\u001b[34m2023-08-22 14:38:11,477 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[34m08/22/2023 14:38:15 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\u001b[0m\n",
      "\u001b[34m08/22/2023 14:38:15 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\u001b[0m\n",
      "\u001b[34m_n_gpu=1,\u001b[0m\n",
      "\u001b[34madafactor=False,\u001b[0m\n",
      "\u001b[34madam_beta1=0.9,\u001b[0m\n",
      "\u001b[34madam_beta2=0.999,\u001b[0m\n",
      "\u001b[34madam_epsilon=1e-08,\u001b[0m\n",
      "\u001b[34mauto_find_batch_size=False,\u001b[0m\n",
      "\u001b[34mbf16=False,\u001b[0m\n",
      "\u001b[34mbf16_full_eval=False,\u001b[0m\n",
      "\u001b[34mdata_seed=None,\u001b[0m\n",
      "\u001b[34mdataloader_drop_last=False,\u001b[0m\n",
      "\u001b[34mdataloader_num_workers=0,\u001b[0m\n",
      "\u001b[34mdataloader_pin_memory=True,\u001b[0m\n",
      "\u001b[34mddp_bucket_cap_mb=None,\u001b[0m\n",
      "\u001b[34mddp_find_unused_parameters=None,\u001b[0m\n",
      "\u001b[34mddp_timeout=1800,\u001b[0m\n",
      "\u001b[34mdebug=[],\u001b[0m\n",
      "\u001b[34mdeepspeed=None,\u001b[0m\n",
      "\u001b[34mdisable_tqdm=False,\u001b[0m\n",
      "\u001b[34mdo_eval=True,\u001b[0m\n",
      "\u001b[34mdo_predict=False,\u001b[0m\n",
      "\u001b[34mdo_train=True,\u001b[0m\n",
      "\u001b[34meval_accumulation_steps=None,\u001b[0m\n",
      "\u001b[34meval_delay=0,\u001b[0m\n",
      "\u001b[34meval_steps=None,\u001b[0m\n",
      "\u001b[34mevaluation_strategy=no,\u001b[0m\n",
      "\u001b[34mfp16=False,\u001b[0m\n",
      "\u001b[34mfp16_backend=auto,\u001b[0m\n",
      "\u001b[34mfp16_full_eval=False,\u001b[0m\n",
      "\u001b[34mfp16_opt_level=O1,\u001b[0m\n",
      "\u001b[34mfsdp=[],\u001b[0m\n",
      "\u001b[34mfsdp_min_num_params=0,\u001b[0m\n",
      "\u001b[34mfsdp_transformer_layer_cls_to_wrap=None,\u001b[0m\n",
      "\u001b[34mfull_determinism=False,\u001b[0m\n",
      "\u001b[34mgradient_accumulation_steps=1,\u001b[0m\n",
      "\u001b[34mgradient_checkpointing=False,\u001b[0m\n",
      "\u001b[34mgreater_is_better=None,\u001b[0m\n",
      "\u001b[34mgroup_by_length=False,\u001b[0m\n",
      "\u001b[34mhalf_precision_backend=auto,\u001b[0m\n",
      "\u001b[34mhub_model_id=None,\u001b[0m\n",
      "\u001b[34mhub_private_repo=False,\u001b[0m\n",
      "\u001b[34mhub_strategy=every_save,\u001b[0m\n",
      "\u001b[34mhub_token=<HUB_TOKEN>,\u001b[0m\n",
      "\u001b[34mignore_data_skip=False,\u001b[0m\n",
      "\u001b[34minclude_inputs_for_metrics=False,\u001b[0m\n",
      "\u001b[34mjit_mode_eval=False,\u001b[0m\n",
      "\u001b[34mlabel_names=None,\u001b[0m\n",
      "\u001b[34mlabel_smoothing_factor=0.0,\u001b[0m\n",
      "\u001b[34mlearning_rate=5e-05,\u001b[0m\n",
      "\u001b[34mlength_column_name=length,\u001b[0m\n",
      "\u001b[34mload_best_model_at_end=False,\u001b[0m\n",
      "\u001b[34mlocal_rank=-1,\u001b[0m\n",
      "\u001b[34mlog_level=passive,\u001b[0m\n",
      "\u001b[34mlog_level_replica=passive,\u001b[0m\n",
      "\u001b[34mlog_on_each_node=True,\u001b[0m\n",
      "\u001b[34mlogging_dir=/opt/ml/model/runs/Aug22_14-38-15_algo-1,\u001b[0m\n",
      "\u001b[34mlogging_first_step=False,\u001b[0m\n",
      "\u001b[34mlogging_nan_inf_filter=True,\u001b[0m\n",
      "\u001b[34mlogging_steps=500,\u001b[0m\n",
      "\u001b[34mlogging_strategy=steps,\u001b[0m\n",
      "\u001b[34mlr_scheduler_type=linear,\u001b[0m\n",
      "\u001b[34mmax_grad_norm=1.0,\u001b[0m\n",
      "\u001b[34mmax_steps=-1,\u001b[0m\n",
      "\u001b[34mmetric_for_best_model=None,\u001b[0m\n",
      "\u001b[34mmp_parameters=,\u001b[0m\n",
      "\u001b[34mno_cuda=False,\u001b[0m\n",
      "\u001b[34mnum_train_epochs=1.0,\u001b[0m\n",
      "\u001b[34moptim=adamw_hf,\u001b[0m\n",
      "\u001b[34moptim_args=None,\u001b[0m\n",
      "\u001b[34moutput_dir=/opt/ml/model,\u001b[0m\n",
      "\u001b[34moverwrite_output_dir=False,\u001b[0m\n",
      "\u001b[34mpast_index=-1,\u001b[0m\n",
      "\u001b[34mper_device_eval_batch_size=8,\u001b[0m\n",
      "\u001b[34mper_device_train_batch_size=8,\u001b[0m\n",
      "\u001b[34mprediction_loss_only=False,\u001b[0m\n",
      "\u001b[34mpush_to_hub=False,\u001b[0m\n",
      "\u001b[34mpush_to_hub_model_id=None,\u001b[0m\n",
      "\u001b[34mpush_to_hub_organization=None,\u001b[0m\n",
      "\u001b[34mpush_to_hub_token=<PUSH_TO_HUB_TOKEN>,\u001b[0m\n",
      "\u001b[34mray_scope=last,\u001b[0m\n",
      "\u001b[34mremove_unused_columns=True,\u001b[0m\n",
      "\u001b[34mreport_to=[],\u001b[0m\n",
      "\u001b[34mresume_from_checkpoint=None,\u001b[0m\n",
      "\u001b[34mrun_name=/opt/ml/model,\u001b[0m\n",
      "\u001b[34msave_on_each_node=False,\u001b[0m\n",
      "\u001b[34msave_steps=500,\u001b[0m\n",
      "\u001b[34msave_strategy=steps,\u001b[0m\n",
      "\u001b[34msave_total_limit=None,\u001b[0m\n",
      "\u001b[34mseed=42,\u001b[0m\n",
      "\u001b[34msharded_ddp=[],\u001b[0m\n",
      "\u001b[34mskip_memory_metrics=True,\u001b[0m\n",
      "\u001b[34mtf32=None,\u001b[0m\n",
      "\u001b[34mtorch_compile=False,\u001b[0m\n",
      "\u001b[34mtorch_compile_backend=None,\u001b[0m\n",
      "\u001b[34mtorch_compile_mode=None,\u001b[0m\n",
      "\u001b[34mtorchdynamo=None,\u001b[0m\n",
      "\u001b[34mtpu_metrics_debug=False,\u001b[0m\n",
      "\u001b[34mtpu_num_cores=None,\u001b[0m\n",
      "\u001b[34muse_ipex=False,\u001b[0m\n",
      "\u001b[34muse_legacy_prediction_loop=False,\u001b[0m\n",
      "\u001b[34muse_mps_device=False,\u001b[0m\n",
      "\u001b[34mwarmup_ratio=0.0,\u001b[0m\n",
      "\u001b[34mwarmup_steps=0,\u001b[0m\n",
      "\u001b[34mweight_decay=0.0,\u001b[0m\n",
      "\u001b[34mxpu_backend=None,\u001b[0m\n",
      "\u001b[34m)\u001b[0m\n",
      "\u001b[34m08/22/2023 14:38:15 - INFO - datasets.utils.file_utils - https://huggingface.co/datasets/wnut_17/resolve/main/wnut_17.py not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmp8wnh4phq\u001b[0m\n",
      "\u001b[34mDownloading builder script:   0%|          | 0.00/7.46k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading builder script: 100%|ââââââââââ| 7.46k/7.46k [00:00<00:00, 4.99MB/s]\u001b[0m\n",
      "\u001b[34m08/22/2023 14:38:15 - INFO - datasets.utils.file_utils - storing https://huggingface.co/datasets/wnut_17/resolve/main/wnut_17.py in cache at /root/.cache/huggingface/datasets/downloads/cb8d3189178ffd9a3e0220867b361cdf59515542cd5c1db0f6006f01b9535d68.b07bfe217645b44e29826533bdfe4cd6816f8d741f857a859e46518a167ade29.py\u001b[0m\n",
      "\u001b[34m08/22/2023 14:38:15 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/cb8d3189178ffd9a3e0220867b361cdf59515542cd5c1db0f6006f01b9535d68.b07bfe217645b44e29826533bdfe4cd6816f8d741f857a859e46518a167ade29.py\u001b[0m\n",
      "\u001b[34m08/22/2023 14:38:15 - INFO - datasets.utils.file_utils - https://huggingface.co/datasets/wnut_17/resolve/main/dataset_infos.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmppejezjzs\u001b[0m\n",
      "\u001b[34mDownloading metadata:   0%|          | 0.00/4.28k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading metadata: 100%|ââââââââââ| 4.28k/4.28k [00:00<00:00, 2.85MB/s]\u001b[0m\n",
      "\u001b[34m08/22/2023 14:38:15 - INFO - datasets.utils.file_utils - storing https://huggingface.co/datasets/wnut_17/resolve/main/dataset_infos.json in cache at /root/.cache/huggingface/datasets/downloads/9a25be179d8df8fde41e5baf597b8e9f5690a2316f4d2b06f8abe9b1d5359b33.c61b848cf9b3aba5b793ac61a125d5e65ccd3c5d4849f81c057e191df14e3d9b\u001b[0m\n",
      "\u001b[34m08/22/2023 14:38:15 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/9a25be179d8df8fde41e5baf597b8e9f5690a2316f4d2b06f8abe9b1d5359b33.c61b848cf9b3aba5b793ac61a125d5e65ccd3c5d4849f81c057e191df14e3d9b\u001b[0m\n",
      "\u001b[34m08/22/2023 14:38:15 - INFO - datasets.utils.file_utils - https://huggingface.co/datasets/wnut_17/resolve/main/README.md not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmprieazymr\u001b[0m\n",
      "\u001b[34mDownloading readme:   0%|          | 0.00/9.05k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading readme: 100%|ââââââââââ| 9.05k/9.05k [00:00<00:00, 6.79MB/s]\u001b[0m\n",
      "\u001b[34m08/22/2023 14:38:15 - INFO - datasets.utils.file_utils - storing https://huggingface.co/datasets/wnut_17/resolve/main/README.md in cache at /root/.cache/huggingface/datasets/downloads/6c71f17bfba45b9dd0381a6ef5fb312c34c7d039c23b3a9dc59bb2eb9c744a98.dba18617ae1987cc9dc3a85f4efa8f4926f7b4dbddce790a19f128730ad228a9\u001b[0m\n",
      "\u001b[34m08/22/2023 14:38:15 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/6c71f17bfba45b9dd0381a6ef5fb312c34c7d039c23b3a9dc59bb2eb9c744a98.dba18617ae1987cc9dc3a85f4efa8f4926f7b4dbddce790a19f128730ad228a9\u001b[0m\n",
      "\u001b[34m08/22/2023 14:38:15 - INFO - datasets.builder - No config specified, defaulting to the single config: wnut_17/wnut_17\u001b[0m\n",
      "\u001b[34m08/22/2023 14:38:15 - INFO - datasets.info - Loading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/wnut_17/077c7f08b8dbc800692e8c9186cdf3606d5849ab0e7be662e6135bb10eba54f9\u001b[0m\n",
      "\u001b[34m08/22/2023 14:38:16 - INFO - datasets.builder - Generating dataset wnut_17 (/root/.cache/huggingface/datasets/wnut_17/wnut_17/1.0.0/077c7f08b8dbc800692e8c9186cdf3606d5849ab0e7be662e6135bb10eba54f9)\u001b[0m\n",
      "\u001b[34mDownloading and preparing dataset wnut_17/wnut_17 to /root/.cache/huggingface/datasets/wnut_17/wnut_17/1.0.0/077c7f08b8dbc800692e8c9186cdf3606d5849ab0e7be662e6135bb10eba54f9...\u001b[0m\n",
      "\u001b[34m08/22/2023 14:38:16 - INFO - datasets.builder - Dataset not on Hf google storage. Downloading and preparing it from source\u001b[0m\n",
      "\u001b[34mDownloading data files:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m08/22/2023 14:38:16 - INFO - datasets.utils.file_utils - https://raw.githubusercontent.com/leondz/emerging_entities_17/master/wnut17train.conll not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmpqgg5ftm2\u001b[0m\n",
      "\u001b[34mDownloading data:   0%|          | 0.00/185k [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading data: 494kB [00:00, 10.4MB/s]\u001b[0m\n",
      "\u001b[34m08/22/2023 14:38:16 - INFO - datasets.utils.file_utils - storing https://raw.githubusercontent.com/leondz/emerging_entities_17/master/wnut17train.conll in cache at /root/.cache/huggingface/datasets/downloads/1c663116ec11ffb0a6f2518c6846086c9e62916c887d0cff71fff8933111533f\u001b[0m\n",
      "\u001b[34m08/22/2023 14:38:16 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/1c663116ec11ffb0a6f2518c6846086c9e62916c887d0cff71fff8933111533f\u001b[0m\n",
      "\u001b[34mDownloading data files:  33%|ââââ      | 1/3 [00:00<00:00,  2.17it/s]\u001b[0m\n",
      "\u001b[34m08/22/2023 14:38:16 - INFO - datasets.utils.file_utils - https://raw.githubusercontent.com/leondz/emerging_entities_17/master/emerging.dev.conll not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmptygb8xxj\u001b[0m\n",
      "\u001b[34mDownloading data:   0%|          | 0.00/39.1k [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading data: 115kB [00:00, 8.52MB/s]\u001b[0m\n",
      "\u001b[34m08/22/2023 14:38:16 - INFO - datasets.utils.file_utils - storing https://raw.githubusercontent.com/leondz/emerging_entities_17/master/emerging.dev.conll in cache at /root/.cache/huggingface/datasets/downloads/8bf465467e300aa565a240a1c61a226ed3f4b05bf675f8b27c0f51cce5a3fb79\u001b[0m\n",
      "\u001b[34m08/22/2023 14:38:16 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/8bf465467e300aa565a240a1c61a226ed3f4b05bf675f8b27c0f51cce5a3fb79\u001b[0m\n",
      "\u001b[34mDownloading data files:  67%|âââââââ   | 2/3 [00:00<00:00,  3.43it/s]\u001b[0m\n",
      "\u001b[34m08/22/2023 14:38:16 - INFO - datasets.utils.file_utils - https://raw.githubusercontent.com/leondz/emerging_entities_17/master/emerging.test.annotated not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmp_8al2iq1\u001b[0m\n",
      "\u001b[34mDownloading data:   0%|          | 0.00/66.9k [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading data: 192kB [00:00, 5.43MB/s]\u001b[0m\n",
      "\u001b[34m08/22/2023 14:38:16 - INFO - datasets.utils.file_utils - storing https://raw.githubusercontent.com/leondz/emerging_entities_17/master/emerging.test.annotated in cache at /root/.cache/huggingface/datasets/downloads/44b136748a9f7a18a25ffe3556a6d0ece455872c29794ab3fd9eb692012f86b0\u001b[0m\n",
      "\u001b[34m08/22/2023 14:38:16 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/44b136748a9f7a18a25ffe3556a6d0ece455872c29794ab3fd9eb692012f86b0\u001b[0m\n",
      "\u001b[34mDownloading data files: 100%|ââââââââââ| 3/3 [00:00<00:00,  3.72it/s]\u001b[0m\n",
      "\u001b[34mDownloading data files: 100%|ââââââââââ| 3/3 [00:00<00:00,  3.43it/s]\u001b[0m\n",
      "\u001b[34m08/22/2023 14:38:16 - INFO - datasets.download.download_manager - Downloading took 0.0 min\u001b[0m\n",
      "\u001b[34m08/22/2023 14:38:16 - INFO - datasets.download.download_manager - Checksum Computation took 0.0 min\u001b[0m\n",
      "\u001b[34mExtracting data files:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mExtracting data files: 100%|ââââââââââ| 3/3 [00:00<00:00, 1669.26it/s]\u001b[0m\n",
      "\u001b[34m08/22/2023 14:38:16 - INFO - datasets.utils.info_utils - Unable to verify checksums.\u001b[0m\n",
      "\u001b[34m08/22/2023 14:38:16 - INFO - datasets.builder - Generating train split\u001b[0m\n",
      "\u001b[34mGenerating train split:   0%|          | 0/3394 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split:  21%|ââ        | 708/3394 [00:00<00:00, 7048.06 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split:  50%|âââââ     | 1708/3394 [00:00<00:00, 4673.34 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split:  71%|ââââââââ  | 2420/3394 [00:00<00:00, 5427.29 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split:  94%|ââââââââââ| 3198/3394 [00:00<00:00, 6144.41 examples/s]\u001b[0m\n",
      "\u001b[34m08/22/2023 14:38:17 - INFO - datasets.builder - Generating validation split\u001b[0m\n",
      "\u001b[34mGenerating validation split:   0%|          | 0/1009 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mGenerating validation split:  86%|âââââââââ | 870/1009 [00:00<00:00, 8665.59 examples/s]\u001b[0m\n",
      "\u001b[34m08/22/2023 14:38:17 - INFO - datasets.builder - Generating test split\u001b[0m\n",
      "\u001b[34mGenerating test split:   0%|          | 0/1287 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mGenerating test split:  59%|ââââââ    | 758/1287 [00:00<00:00, 7547.65 examples/s]\u001b[0m\n",
      "\u001b[34m08/22/2023 14:38:17 - INFO - datasets.utils.info_utils - All the splits matched successfully.\u001b[0m\n",
      "\u001b[34mDataset wnut_17 downloaded and prepared to /root/.cache/huggingface/datasets/wnut_17/wnut_17/1.0.0/077c7f08b8dbc800692e8c9186cdf3606d5849ab0e7be662e6135bb10eba54f9. Subsequent calls will reuse this data.\u001b[0m\n",
      "\u001b[34m0%|          | 0/3 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m100%|ââââââââââ| 3/3 [00:00<00:00, 673.24it/s]\u001b[0m\n",
      "\u001b[34mDownloading (â¦)lve/main/config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading (â¦)lve/main/config.json: 100%|ââââââââââ| 483/483 [00:00<00:00, 58.3kB/s]\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:660] 2023-08-22 14:38:17,993 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/6cdc0aad91f5ae2e6712e91bc7b65d1cf5c05411/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:660] 2023-08-22 14:38:17,993 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/6cdc0aad91f5ae2e6712e91bc7b65d1cf5c05411/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:712] 2023-08-22 14:38:17,997 >> Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"finetuning_task\": \"ner\",\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\",\n",
      "    \"9\": \"LABEL_9\",\n",
      "    \"10\": \"LABEL_10\",\n",
      "    \"11\": \"LABEL_11\",\n",
      "    \"12\": \"LABEL_12\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_10\": 10,\n",
      "    \"LABEL_11\": 11,\n",
      "    \"LABEL_12\": 12,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8,\n",
      "    \"LABEL_9\": 9\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.26.0\",\n",
      "  \"vocab_size\": 30522\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:712] 2023-08-22 14:38:17,997 >> Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"finetuning_task\": \"ner\",\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\",\n",
      "    \"9\": \"LABEL_9\",\n",
      "    \"10\": \"LABEL_10\",\n",
      "    \"11\": \"LABEL_11\",\n",
      "    \"12\": \"LABEL_12\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_10\": 10,\n",
      "    \"LABEL_11\": 11,\n",
      "    \"LABEL_12\": 12,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8,\n",
      "    \"LABEL_9\": 9\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.26.0\",\n",
      "  \"vocab_size\": 30522\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mDownloading (â¦)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading (â¦)okenizer_config.json: 100%|ââââââââââ| 28.0/28.0 [00:00<00:00, 3.66kB/s]\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:660] 2023-08-22 14:38:18,120 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/6cdc0aad91f5ae2e6712e91bc7b65d1cf5c05411/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:660] 2023-08-22 14:38:18,120 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/6cdc0aad91f5ae2e6712e91bc7b65d1cf5c05411/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:712] 2023-08-22 14:38:18,121 >> Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.26.0\",\n",
      "  \"vocab_size\": 30522\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:712] 2023-08-22 14:38:18,121 >> Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.26.0\",\n",
      "  \"vocab_size\": 30522\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mDownloading (â¦)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading (â¦)solve/main/vocab.txt: 100%|ââââââââââ| 232k/232k [00:00<00:00, 37.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading (â¦)/main/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading (â¦)/main/tokenizer.json: 100%|ââââââââââ| 466k/466k [00:00<00:00, 74.9MB/s]\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1802] 2023-08-22 14:38:18,485 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/6cdc0aad91f5ae2e6712e91bc7b65d1cf5c05411/vocab.txt\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1802] 2023-08-22 14:38:18,485 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/6cdc0aad91f5ae2e6712e91bc7b65d1cf5c05411/tokenizer.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1802] 2023-08-22 14:38:18,485 >> loading file added_tokens.json from cache at None\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1802] 2023-08-22 14:38:18,485 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/6cdc0aad91f5ae2e6712e91bc7b65d1cf5c05411/vocab.txt\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1802] 2023-08-22 14:38:18,485 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/6cdc0aad91f5ae2e6712e91bc7b65d1cf5c05411/tokenizer.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1802] 2023-08-22 14:38:18,485 >> loading file added_tokens.json from cache at None\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1802] 2023-08-22 14:38:18,485 >> loading file special_tokens_map.json from cache at None\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1802] 2023-08-22 14:38:18,485 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/6cdc0aad91f5ae2e6712e91bc7b65d1cf5c05411/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1802] 2023-08-22 14:38:18,485 >> loading file special_tokens_map.json from cache at None\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1802] 2023-08-22 14:38:18,485 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/6cdc0aad91f5ae2e6712e91bc7b65d1cf5c05411/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:660] 2023-08-22 14:38:18,486 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/6cdc0aad91f5ae2e6712e91bc7b65d1cf5c05411/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:660] 2023-08-22 14:38:18,486 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/6cdc0aad91f5ae2e6712e91bc7b65d1cf5c05411/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:712] 2023-08-22 14:38:18,486 >> Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.26.0\",\n",
      "  \"vocab_size\": 30522\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:712] 2023-08-22 14:38:18,486 >> Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.26.0\",\n",
      "  \"vocab_size\": 30522\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mDownloading (â¦)\"pytorch_model.bin\";:   0%|          | 0.00/268M [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading (â¦)\"pytorch_model.bin\";:  16%|ââ        | 41.9M/268M [00:00<00:00, 378MB/s]\u001b[0m\n",
      "\u001b[34mDownloading (â¦)\"pytorch_model.bin\";:  35%|ââââ      | 94.4M/268M [00:00<00:00, 409MB/s]\u001b[0m\n",
      "\u001b[34mDownloading (â¦)\"pytorch_model.bin\";:  55%|ââââââ    | 147M/268M [00:00<00:00, 432MB/s]\u001b[0m\n",
      "\u001b[34mDownloading (â¦)\"pytorch_model.bin\";:  74%|ââââââââ  | 199M/268M [00:00<00:00, 444MB/s]\u001b[0m\n",
      "\u001b[34mDownloading (â¦)\"pytorch_model.bin\";:  94%|ââââââââââ| 252M/268M [00:00<00:00, 450MB/s]\u001b[0m\n",
      "\u001b[34mDownloading (â¦)\"pytorch_model.bin\";: 100%|ââââââââââ| 268M/268M [00:00<00:00, 434MB/s]\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:2275] 2023-08-22 14:38:19,277 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/6cdc0aad91f5ae2e6712e91bc7b65d1cf5c05411/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:2275] 2023-08-22 14:38:19,277 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/6cdc0aad91f5ae2e6712e91bc7b65d1cf5c05411/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[WARNING|modeling_utils.py:2847] 2023-08-22 14:38:20,148 >> Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForTokenClassification: ['vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_transform.bias']\u001b[0m\n",
      "\u001b[34m- This IS expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m- This IS NOT expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[WARNING|modeling_utils.py:2859] 2023-08-22 14:38:20,148 >> Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[WARNING|modeling_utils.py:2847] 2023-08-22 14:38:20,148 >> Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForTokenClassification: ['vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_transform.bias']\u001b[0m\n",
      "\u001b[34m- This IS expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m- This IS NOT expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[WARNING|modeling_utils.py:2859] 2023-08-22 14:38:20,148 >> Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   0%|          | 0/4 [00:00<?, ?ba/s]\u001b[0m\n",
      "\u001b[34m08/22/2023 14:38:20 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/wnut_17/wnut_17/1.0.0/077c7f08b8dbc800692e8c9186cdf3606d5849ab0e7be662e6135bb10eba54f9/cache-9fe3fef6ef62b171.arrow\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  25%|âââ       | 1/4 [00:00<00:00,  9.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  75%|ââââââââ  | 3/4 [00:00<00:00, 10.02ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset: 100%|ââââââââââ| 4/4 [00:00<00:00, 11.70ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on validation dataset:   0%|          | 0/2 [00:00<?, ?ba/s]\u001b[0m\n",
      "\u001b[34m08/22/2023 14:38:20 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/wnut_17/wnut_17/1.0.0/077c7f08b8dbc800692e8c9186cdf3606d5849ab0e7be662e6135bb10eba54f9/cache-ed293169f9c446f8.arrow\u001b[0m\n",
      "\u001b[34mRunning tokenizer on validation dataset: 100%|ââââââââââ| 2/2 [00:00<00:00, 24.42ba/s]\u001b[0m\n",
      "\u001b[34mDownloading builder script:   0%|          | 0.00/6.34k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading builder script: 100%|ââââââââââ| 6.34k/6.34k [00:00<00:00, 5.42MB/s]\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:710] 2023-08-22 14:38:22,334 >> The following columns in the training set don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: ner_tags, tokens, id. If ner_tags, tokens, id are not expected by `DistilBertForTokenClassification.forward`,  you can safely ignore this message.\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:710] 2023-08-22 14:38:22,334 >> The following columns in the training set don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: ner_tags, tokens, id. If ner_tags, tokens, id are not expected by `DistilBertForTokenClassification.forward`,  you can safely ignore this message.\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1650] 2023-08-22 14:38:22,344 >> ***** Running training *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1650] 2023-08-22 14:38:22,344 >> ***** Running training *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1651] 2023-08-22 14:38:22,344 >>   Num examples = 3394\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1652] 2023-08-22 14:38:22,344 >>   Num Epochs = 1\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1651] 2023-08-22 14:38:22,344 >>   Num examples = 3394\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1652] 2023-08-22 14:38:22,344 >>   Num Epochs = 1\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1653] 2023-08-22 14:38:22,345 >>   Instantaneous batch size per device = 8\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1654] 2023-08-22 14:38:22,345 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1653] 2023-08-22 14:38:22,345 >>   Instantaneous batch size per device = 8\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1654] 2023-08-22 14:38:22,345 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1655] 2023-08-22 14:38:22,345 >>   Gradient Accumulation steps = 1\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1656] 2023-08-22 14:38:22,345 >>   Total optimization steps = 425\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1655] 2023-08-22 14:38:22,345 >>   Gradient Accumulation steps = 1\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1656] 2023-08-22 14:38:22,345 >>   Total optimization steps = 425\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1657] 2023-08-22 14:38:22,345 >>   Number of trainable parameters = 66372877\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1657] 2023-08-22 14:38:22,345 >>   Number of trainable parameters = 66372877\u001b[0m\n",
      "\u001b[34m0%|          | 0/425 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[2023-08-22 14:38:22.460: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.26.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[2023-08-22 14:38:22.503 algo-1:68 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2023-08-22 14:38:22.545 algo-1:68 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2023-08-22 14:38:22.546 algo-1:68 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2023-08-22 14:38:22.546 algo-1:68 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2023-08-22 14:38:22.547 algo-1:68 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2023-08-22 14:38:22.547 algo-1:68 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[WARNING|logging.py:281] 2023-08-22 14:38:22,550 >> You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34m[WARNING|logging.py:281] 2023-08-22 14:38:22,550 >> You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34m0%|          | 1/425 [00:01<09:10,  1.30s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 4/425 [00:01<01:58,  3.55it/s]\u001b[0m\n",
      "\u001b[34m2%|â         | 7/425 [00:01<01:04,  6.45it/s]\u001b[0m\n",
      "\u001b[34m2%|â         | 10/425 [00:01<00:44,  9.31it/s]\u001b[0m\n",
      "\u001b[34m3%|â         | 13/425 [00:01<00:33, 12.28it/s]\u001b[0m\n",
      "\u001b[34m4%|â         | 16/425 [00:01<00:27, 15.10it/s]\u001b[0m\n",
      "\u001b[34m4%|â         | 19/425 [00:02<00:23, 17.49it/s]\u001b[0m\n",
      "\u001b[34m5%|â         | 22/425 [00:02<00:20, 19.34it/s]\u001b[0m\n",
      "\u001b[34m6%|â         | 25/425 [00:02<00:19, 20.36it/s]\u001b[0m\n",
      "\u001b[34m7%|â         | 28/425 [00:02<00:18, 21.06it/s]\u001b[0m\n",
      "\u001b[34m7%|â         | 31/425 [00:02<00:17, 21.95it/s]\u001b[0m\n",
      "\u001b[34m8%|â         | 34/425 [00:02<00:17, 22.48it/s]\u001b[0m\n",
      "\u001b[34m9%|â         | 37/425 [00:02<00:16, 22.98it/s]\u001b[0m\n",
      "\u001b[34m9%|â         | 40/425 [00:02<00:16, 23.78it/s]\u001b[0m\n",
      "\u001b[34m10%|â         | 43/425 [00:03<00:15, 24.49it/s]\u001b[0m\n",
      "\u001b[34m11%|â         | 46/425 [00:03<00:15, 25.11it/s]\u001b[0m\n",
      "\u001b[34m12%|ââ        | 49/425 [00:03<00:14, 25.42it/s]\u001b[0m\n",
      "\u001b[34m12%|ââ        | 52/425 [00:03<00:14, 25.46it/s]\u001b[0m\n",
      "\u001b[34m13%|ââ        | 55/425 [00:03<00:14, 25.33it/s]\u001b[0m\n",
      "\u001b[34m14%|ââ        | 58/425 [00:03<00:14, 24.91it/s]\u001b[0m\n",
      "\u001b[34m14%|ââ        | 61/425 [00:03<00:14, 25.54it/s]\u001b[0m\n",
      "\u001b[34m15%|ââ        | 64/425 [00:03<00:14, 25.54it/s]\u001b[0m\n",
      "\u001b[34m16%|ââ        | 67/425 [00:03<00:14, 25.32it/s]\u001b[0m\n",
      "\u001b[34m16%|ââ        | 70/425 [00:04<00:14, 24.73it/s]\u001b[0m\n",
      "\u001b[34m17%|ââ        | 73/425 [00:04<00:14, 24.73it/s]\u001b[0m\n",
      "\u001b[34m18%|ââ        | 76/425 [00:04<00:13, 25.14it/s]\u001b[0m\n",
      "\u001b[34m19%|ââ        | 79/425 [00:04<00:13, 25.20it/s]\u001b[0m\n",
      "\u001b[34m19%|ââ        | 82/425 [00:04<00:13, 24.60it/s]\u001b[0m\n",
      "\u001b[34m20%|ââ        | 85/425 [00:04<00:13, 24.77it/s]\u001b[0m\n",
      "\u001b[34m21%|ââ        | 88/425 [00:04<00:13, 24.46it/s]\u001b[0m\n",
      "\u001b[34m21%|âââ       | 91/425 [00:04<00:13, 24.26it/s]\u001b[0m\n",
      "\u001b[34m22%|âââ       | 94/425 [00:05<00:13, 24.62it/s]\u001b[0m\n",
      "\u001b[34m23%|âââ       | 97/425 [00:05<00:13, 24.68it/s]\u001b[0m\n",
      "\u001b[34m24%|âââ       | 100/425 [00:05<00:13, 24.52it/s]\u001b[0m\n",
      "\u001b[34m24%|âââ       | 103/425 [00:05<00:13, 24.39it/s]\u001b[0m\n",
      "\u001b[34m25%|âââ       | 106/425 [00:05<00:13, 24.44it/s]\u001b[0m\n",
      "\u001b[34m26%|âââ       | 109/425 [00:05<00:12, 25.01it/s]\u001b[0m\n",
      "\u001b[34m26%|âââ       | 112/425 [00:05<00:12, 25.30it/s]\u001b[0m\n",
      "\u001b[34m27%|âââ       | 115/425 [00:05<00:12, 25.52it/s]\u001b[0m\n",
      "\u001b[34m28%|âââ       | 118/425 [00:06<00:11, 25.69it/s]\u001b[0m\n",
      "\u001b[34m28%|âââ       | 121/425 [00:06<00:11, 25.91it/s]\u001b[0m\n",
      "\u001b[34m29%|âââ       | 124/425 [00:06<00:11, 25.83it/s]\u001b[0m\n",
      "\u001b[34m30%|âââ       | 127/425 [00:06<00:11, 25.81it/s]\u001b[0m\n",
      "\u001b[34m31%|âââ       | 130/425 [00:06<00:11, 25.81it/s]\u001b[0m\n",
      "\u001b[34m31%|ââââ      | 133/425 [00:06<00:11, 25.95it/s]\u001b[0m\n",
      "\u001b[34m32%|ââââ      | 136/425 [00:06<00:11, 26.09it/s]\u001b[0m\n",
      "\u001b[34m33%|ââââ      | 139/425 [00:06<00:10, 26.23it/s]\u001b[0m\n",
      "\u001b[34m33%|ââââ      | 142/425 [00:06<00:10, 26.43it/s]\u001b[0m\n",
      "\u001b[34m34%|ââââ      | 145/425 [00:07<00:10, 25.70it/s]\u001b[0m\n",
      "\u001b[34m35%|ââââ      | 148/425 [00:07<00:10, 25.50it/s]\u001b[0m\n",
      "\u001b[34m36%|ââââ      | 151/425 [00:07<00:10, 24.93it/s]\u001b[0m\n",
      "\u001b[34m36%|ââââ      | 154/425 [00:07<00:10, 25.38it/s]\u001b[0m\n",
      "\u001b[34m37%|ââââ      | 157/425 [00:07<00:10, 25.41it/s]\u001b[0m\n",
      "\u001b[34m38%|ââââ      | 160/425 [00:07<00:10, 24.72it/s]\u001b[0m\n",
      "\u001b[34m38%|ââââ      | 163/425 [00:07<00:10, 24.59it/s]\u001b[0m\n",
      "\u001b[34m39%|ââââ      | 166/425 [00:07<00:10, 24.55it/s]\u001b[0m\n",
      "\u001b[34m40%|ââââ      | 169/425 [00:08<00:10, 24.42it/s]\u001b[0m\n",
      "\u001b[34m40%|ââââ      | 172/425 [00:08<00:10, 24.15it/s]\u001b[0m\n",
      "\u001b[34m41%|ââââ      | 175/425 [00:08<00:10, 24.39it/s]\u001b[0m\n",
      "\u001b[34m42%|âââââ     | 178/425 [00:08<00:10, 24.35it/s]\u001b[0m\n",
      "\u001b[34m43%|âââââ     | 181/425 [00:08<00:09, 24.61it/s]\u001b[0m\n",
      "\u001b[34m43%|âââââ     | 184/425 [00:08<00:09, 24.18it/s]\u001b[0m\n",
      "\u001b[34m44%|âââââ     | 187/425 [00:08<00:09, 23.82it/s]\u001b[0m\n",
      "\u001b[34m45%|âââââ     | 190/425 [00:08<00:10, 23.26it/s]\u001b[0m\n",
      "\u001b[34m45%|âââââ     | 193/425 [00:09<00:09, 23.35it/s]\u001b[0m\n",
      "\u001b[34m46%|âââââ     | 196/425 [00:09<00:09, 23.11it/s]\u001b[0m\n",
      "\u001b[34m47%|âââââ     | 199/425 [00:09<00:09, 23.26it/s]\u001b[0m\n",
      "\u001b[34m48%|âââââ     | 202/425 [00:09<00:09, 23.53it/s]\u001b[0m\n",
      "\u001b[34m48%|âââââ     | 205/425 [00:09<00:09, 24.03it/s]\u001b[0m\n",
      "\u001b[34m49%|âââââ     | 208/425 [00:09<00:08, 24.63it/s]\u001b[0m\n",
      "\u001b[34m50%|âââââ     | 211/425 [00:09<00:08, 24.65it/s]\u001b[0m\n",
      "\u001b[34m50%|âââââ     | 214/425 [00:09<00:08, 24.96it/s]\u001b[0m\n",
      "\u001b[34m51%|âââââ     | 217/425 [00:10<00:08, 24.49it/s]\u001b[0m\n",
      "\u001b[34m52%|ââââââ    | 220/425 [00:10<00:08, 24.23it/s]\u001b[0m\n",
      "\u001b[34m52%|ââââââ    | 223/425 [00:10<00:08, 23.98it/s]\u001b[0m\n",
      "\u001b[34m53%|ââââââ    | 226/425 [00:10<00:08, 24.14it/s]\u001b[0m\n",
      "\u001b[34m54%|ââââââ    | 229/425 [00:10<00:08, 24.08it/s]\u001b[0m\n",
      "\u001b[34m55%|ââââââ    | 232/425 [00:10<00:08, 23.95it/s]\u001b[0m\n",
      "\u001b[34m55%|ââââââ    | 235/425 [00:10<00:07, 24.33it/s]\u001b[0m\n",
      "\u001b[34m56%|ââââââ    | 238/425 [00:10<00:07, 24.97it/s]\u001b[0m\n",
      "\u001b[34m57%|ââââââ    | 241/425 [00:11<00:07, 25.36it/s]\u001b[0m\n",
      "\u001b[34m57%|ââââââ    | 244/425 [00:11<00:07, 25.75it/s]\u001b[0m\n",
      "\u001b[34m58%|ââââââ    | 247/425 [00:11<00:06, 26.00it/s]\u001b[0m\n",
      "\u001b[34m59%|ââââââ    | 250/425 [00:11<00:06, 26.05it/s]\u001b[0m\n",
      "\u001b[34m60%|ââââââ    | 253/425 [00:11<00:06, 25.67it/s]\u001b[0m\n",
      "\u001b[34m60%|ââââââ    | 256/425 [00:11<00:06, 25.79it/s]\u001b[0m\n",
      "\u001b[34m61%|ââââââ    | 259/425 [00:11<00:06, 25.82it/s]\u001b[0m\n",
      "\u001b[34m62%|âââââââ   | 262/425 [00:11<00:06, 25.95it/s]\u001b[0m\n",
      "\u001b[34m62%|âââââââ   | 265/425 [00:11<00:06, 26.02it/s]\u001b[0m\n",
      "\u001b[34m63%|âââââââ   | 268/425 [00:12<00:06, 25.97it/s]\u001b[0m\n",
      "\u001b[34m64%|âââââââ   | 271/425 [00:12<00:05, 25.92it/s]\u001b[0m\n",
      "\u001b[34m64%|âââââââ   | 274/425 [00:12<00:05, 25.53it/s]\u001b[0m\n",
      "\u001b[34m65%|âââââââ   | 277/425 [00:12<00:05, 25.08it/s]\u001b[0m\n",
      "\u001b[34m66%|âââââââ   | 280/425 [00:12<00:05, 24.93it/s]\u001b[0m\n",
      "\u001b[34m67%|âââââââ   | 283/425 [00:12<00:05, 25.20it/s]\u001b[0m\n",
      "\u001b[34m67%|âââââââ   | 286/425 [00:12<00:05, 25.50it/s]\u001b[0m\n",
      "\u001b[34m68%|âââââââ   | 289/425 [00:12<00:05, 25.87it/s]\u001b[0m\n",
      "\u001b[34m69%|âââââââ   | 292/425 [00:12<00:05, 26.21it/s]\u001b[0m\n",
      "\u001b[34m69%|âââââââ   | 295/425 [00:13<00:04, 26.48it/s]\u001b[0m\n",
      "\u001b[34m70%|âââââââ   | 298/425 [00:13<00:04, 26.05it/s]\u001b[0m\n",
      "\u001b[34m71%|âââââââ   | 301/425 [00:13<00:04, 25.81it/s]\u001b[0m\n",
      "\u001b[34m72%|ââââââââ  | 304/425 [00:13<00:04, 25.76it/s]\u001b[0m\n",
      "\u001b[34m72%|ââââââââ  | 307/425 [00:13<00:04, 25.60it/s]\u001b[0m\n",
      "\u001b[34m73%|ââââââââ  | 310/425 [00:13<00:04, 25.31it/s]\u001b[0m\n",
      "\u001b[34m74%|ââââââââ  | 313/425 [00:13<00:04, 25.25it/s]\u001b[0m\n",
      "\u001b[34m74%|ââââââââ  | 316/425 [00:13<00:04, 25.08it/s]\u001b[0m\n",
      "\u001b[34m75%|ââââââââ  | 319/425 [00:14<00:04, 25.26it/s]\u001b[0m\n",
      "\u001b[34m76%|ââââââââ  | 322/425 [00:14<00:04, 25.72it/s]\u001b[0m\n",
      "\u001b[34m76%|ââââââââ  | 325/425 [00:14<00:03, 25.95it/s]\u001b[0m\n",
      "\u001b[34m77%|ââââââââ  | 328/425 [00:14<00:03, 25.88it/s]\u001b[0m\n",
      "\u001b[34m78%|ââââââââ  | 331/425 [00:14<00:03, 25.58it/s]\u001b[0m\n",
      "\u001b[34m79%|ââââââââ  | 334/425 [00:14<00:03, 26.10it/s]\u001b[0m\n",
      "\u001b[34m79%|ââââââââ  | 337/425 [00:14<00:03, 25.47it/s]\u001b[0m\n",
      "\u001b[34m80%|ââââââââ  | 340/425 [00:14<00:03, 25.65it/s]\u001b[0m\n",
      "\u001b[34m81%|ââââââââ  | 343/425 [00:14<00:03, 25.49it/s]\u001b[0m\n",
      "\u001b[34m81%|âââââââââ | 346/425 [00:15<00:03, 25.19it/s]\u001b[0m\n",
      "\u001b[34m82%|âââââââââ | 349/425 [00:15<00:03, 25.05it/s]\u001b[0m\n",
      "\u001b[34m83%|âââââââââ | 352/425 [00:15<00:02, 25.44it/s]\u001b[0m\n",
      "\u001b[34m84%|âââââââââ | 355/425 [00:15<00:02, 25.20it/s]\u001b[0m\n",
      "\u001b[34m84%|âââââââââ | 358/425 [00:15<00:02, 25.32it/s]\u001b[0m\n",
      "\u001b[34m85%|âââââââââ | 361/425 [00:15<00:02, 25.24it/s]\u001b[0m\n",
      "\u001b[34m86%|âââââââââ | 364/425 [00:15<00:02, 24.63it/s]\u001b[0m\n",
      "\u001b[34m86%|âââââââââ | 367/425 [00:15<00:02, 24.22it/s]\u001b[0m\n",
      "\u001b[34m87%|âââââââââ | 370/425 [00:16<00:02, 23.86it/s]\u001b[0m\n",
      "\u001b[34m88%|âââââââââ | 373/425 [00:16<00:02, 23.79it/s]\u001b[0m\n",
      "\u001b[34m88%|âââââââââ | 376/425 [00:16<00:02, 24.39it/s]\u001b[0m\n",
      "\u001b[34m89%|âââââââââ | 379/425 [00:16<00:01, 24.76it/s]\u001b[0m\n",
      "\u001b[34m90%|âââââââââ | 382/425 [00:16<00:01, 25.03it/s]\u001b[0m\n",
      "\u001b[34m91%|âââââââââ | 385/425 [00:16<00:01, 25.04it/s]\u001b[0m\n",
      "\u001b[34m91%|ââââââââââ| 388/425 [00:16<00:01, 25.22it/s]\u001b[0m\n",
      "\u001b[34m92%|ââââââââââ| 391/425 [00:16<00:01, 25.28it/s]\u001b[0m\n",
      "\u001b[34m93%|ââââââââââ| 394/425 [00:17<00:01, 25.37it/s]\u001b[0m\n",
      "\u001b[34m93%|ââââââââââ| 397/425 [00:17<00:01, 25.17it/s]\u001b[0m\n",
      "\u001b[34m94%|ââââââââââ| 400/425 [00:17<00:01, 24.55it/s]\u001b[0m\n",
      "\u001b[34m95%|ââââââââââ| 403/425 [00:17<00:00, 24.08it/s]\u001b[0m\n",
      "\u001b[34m96%|ââââââââââ| 406/425 [00:17<00:00, 24.22it/s]\u001b[0m\n",
      "\u001b[34m96%|ââââââââââ| 409/425 [00:17<00:00, 23.88it/s]\u001b[0m\n",
      "\u001b[34m97%|ââââââââââ| 412/425 [00:17<00:00, 23.26it/s]\u001b[0m\n",
      "\u001b[34m98%|ââââââââââ| 415/425 [00:17<00:00, 23.00it/s]\u001b[0m\n",
      "\u001b[34m98%|ââââââââââ| 418/425 [00:18<00:00, 23.30it/s]\u001b[0m\n",
      "\u001b[34m99%|ââââââââââ| 421/425 [00:18<00:00, 23.53it/s]\u001b[0m\n",
      "\u001b[34m100%|ââââââââââ| 424/425 [00:18<00:00, 23.94it/s]\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1901] 2023-08-22 14:38:40,698 >> \u001b[0m\n",
      "\u001b[34mTraining completed. Do not forget to share your model on huggingface.co/models =)\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1901] 2023-08-22 14:38:40,698 >> \u001b[0m\n",
      "\u001b[34mTraining completed. Do not forget to share your model on huggingface.co/models =)\u001b[0m\n",
      "\u001b[34m100%|ââââââââââ| 425/425 [00:18<00:00, 23.94it/s]\u001b[0m\n",
      "\u001b[34m{'train_runtime': 18.3533, 'train_samples_per_second': 184.926, 'train_steps_per_second': 23.157, 'train_loss': 0.18624504538143383, 'epoch': 1.0}\u001b[0m\n",
      "\u001b[34m100%|ââââââââââ| 425/425 [00:18<00:00, 23.16it/s]\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2709] 2023-08-22 14:38:40,700 >> Saving model checkpoint to /opt/ml/model\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2709] 2023-08-22 14:38:40,700 >> Saving model checkpoint to /opt/ml/model\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:453] 2023-08-22 14:38:40,701 >> Configuration saved in /opt/ml/model/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:453] 2023-08-22 14:38:40,701 >> Configuration saved in /opt/ml/model/config.json\u001b[0m\n",
      "\n",
      "2023-08-22 14:38:49 Uploading - Uploading generated training model\u001b[34m[INFO|modeling_utils.py:1704] 2023-08-22 14:38:41,123 >> Model weights saved in /opt/ml/model/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1704] 2023-08-22 14:38:41,123 >> Model weights saved in /opt/ml/model/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2160] 2023-08-22 14:38:41,123 >> tokenizer config file saved in /opt/ml/model/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2160] 2023-08-22 14:38:41,123 >> tokenizer config file saved in /opt/ml/model/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2167] 2023-08-22 14:38:41,124 >> Special tokens file saved in /opt/ml/model/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2167] 2023-08-22 14:38:41,124 >> Special tokens file saved in /opt/ml/model/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m***** train metrics *****\u001b[0m\n",
      "\u001b[34mepoch                    =        1.0\n",
      "  train_loss               =     0.1862\n",
      "  train_runtime            = 0:00:18.35\n",
      "  train_samples            =       3394\n",
      "  train_samples_per_second =    184.926\n",
      "  train_steps_per_second   =     23.157\u001b[0m\n",
      "\u001b[34m08/22/2023 14:38:41 - INFO - __main__ - *** Evaluate ***\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:710] 2023-08-22 14:38:41,174 >> The following columns in the evaluation set don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: ner_tags, tokens, id. If ner_tags, tokens, id are not expected by `DistilBertForTokenClassification.forward`,  you can safely ignore this message.\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:710] 2023-08-22 14:38:41,174 >> The following columns in the evaluation set don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: ner_tags, tokens, id. If ner_tags, tokens, id are not expected by `DistilBertForTokenClassification.forward`,  you can safely ignore this message.\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2964] 2023-08-22 14:38:41,176 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2964] 2023-08-22 14:38:41,176 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2966] 2023-08-22 14:38:41,176 >>   Num examples = 1009\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2969] 2023-08-22 14:38:41,176 >>   Batch size = 8\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2966] 2023-08-22 14:38:41,176 >>   Num examples = 1009\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2969] 2023-08-22 14:38:41,176 >>   Batch size = 8\u001b[0m\n",
      "\u001b[34m0%|          | 0/127 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m9%|â         | 11/127 [00:00<00:01, 107.07it/s]\u001b[0m\n",
      "\u001b[34m17%|ââ        | 22/127 [00:00<00:01, 104.18it/s]\u001b[0m\n",
      "\u001b[34m26%|âââ       | 33/127 [00:00<00:00, 102.61it/s]\u001b[0m\n",
      "\u001b[34m35%|ââââ      | 44/127 [00:00<00:00, 101.89it/s]\u001b[0m\n",
      "\u001b[34m43%|âââââ     | 55/127 [00:00<00:00, 101.73it/s]\u001b[0m\n",
      "\u001b[34m52%|ââââââ    | 66/127 [00:00<00:00, 101.99it/s]\u001b[0m\n",
      "\u001b[34m61%|ââââââ    | 77/127 [00:00<00:00, 102.02it/s]\u001b[0m\n",
      "\u001b[34m69%|âââââââ   | 88/127 [00:00<00:00, 101.38it/s]\u001b[0m\n",
      "\u001b[34m78%|ââââââââ  | 99/127 [00:00<00:00, 100.57it/s]\u001b[0m\n",
      "\u001b[34m87%|âââââââââ | 110/127 [00:01<00:00, 100.77it/s]\u001b[0m\n",
      "\u001b[34m95%|ââââââââââ| 121/127 [00:01<00:00, 100.50it/s]\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.9/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\u001b[0m\n",
      "\u001b[34m100%|ââââââââââ| 127/127 [00:01<00:00, 79.14it/s]\u001b[0m\n",
      "\u001b[34m***** eval metrics *****\u001b[0m\n",
      "\u001b[34mepoch                   =        1.0\n",
      "  eval_accuracy           =     0.9494\n",
      "  eval_f1                 =     0.5572\n",
      "  eval_loss               =     0.2312\n",
      "  eval_precision          =     0.6998\n",
      "  eval_recall             =     0.4629\n",
      "  eval_runtime            = 0:00:01.61\n",
      "  eval_samples            =       1009\n",
      "  eval_samples_per_second =    624.202\n",
      "  eval_steps_per_second   =     78.567\u001b[0m\n",
      "\u001b[34m2023-08-22 14:38:43,662 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2023-08-22 14:38:43,662 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2023-08-22 14:38:43,663 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2023-08-22 14:39:15 Completed - Training job completed\n",
      "Training seconds: 318\n",
      "Billable seconds: 318\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.huggingface import HuggingFace\n",
    "\n",
    "hyperparameters = {\n",
    "\t'model_name_or_path':'distilbert-base-uncased',\n",
    "\t'output_dir':'/opt/ml/model',\n",
    "    'dataset_name' : 'wnut_17',\n",
    "    'do_train':'true',\n",
    "    'do_eval':'true',\n",
    "    'num_train_epochs':1\n",
    "}\n",
    "\n",
    "# git configuration to download our fine-tuning script\n",
    "git_config = {'repo': 'https://github.com/huggingface/transformers.git','branch': 'v4.26.0'}\n",
    "\n",
    "# creates Hugging Face estimator\n",
    "huggingface_estimator = HuggingFace(\n",
    "\tentry_point='run_ner.py',\n",
    "\tsource_dir='./examples/pytorch/token-classification',\n",
    "\tinstance_type='ml.p3.2xlarge',\n",
    "\tinstance_count=1,\n",
    "\trole=role,\n",
    "    # training_arguments=training_args,\n",
    "\tgit_config=git_config,\n",
    "\ttransformers_version='4.26.0',\n",
    "\tpytorch_version='1.13.1',\n",
    "\tpy_version='py39',\n",
    "\thyperparameters = hyperparameters,\n",
    ")\n",
    "\n",
    "huggingface_estimator.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating model with name: huggingface-pytorch-training-2023-08-22-20-06-19-654\n",
      "INFO:sagemaker:Creating endpoint-config with name huggingface-pytorch-training-2023-08-22-20-06-19-654\n",
      "INFO:sagemaker:Creating endpoint with name huggingface-pytorch-training-2023-08-22-20-06-19-654\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-b0ca165d6150>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# deploy model to SageMaker Inference\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m predictor = huggingface_estimator.deploy(\n\u001b[0m\u001b[1;32m      3\u001b[0m         \u001b[0minitial_instance_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# number of instances\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0minstance_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ml.m5.xlarge'\u001b[0m \u001b[0;31m# ec2 instance type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m )\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mdeploy\u001b[0;34m(self, initial_instance_count, instance_type, serializer, deserializer, accelerator_type, endpoint_name, use_compiled_model, wait, model_name, kms_key, data_capture_config, tags, serverless_inference_config, async_inference_config, volume_size, model_data_download_timeout, container_startup_health_check_timeout, inference_recommendation_id, explainer_config, **kwargs)\u001b[0m\n\u001b[1;32m   1626\u001b[0m         )\n\u001b[1;32m   1627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1628\u001b[0;31m         return model.deploy(\n\u001b[0m\u001b[1;32m   1629\u001b[0m             \u001b[0minstance_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minstance_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1630\u001b[0m             \u001b[0minitial_instance_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_instance_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/sagemaker/huggingface/model.py\u001b[0m in \u001b[0;36mdeploy\u001b[0;34m(self, initial_instance_count, instance_type, serializer, deserializer, accelerator_type, endpoint_name, tags, kms_key, wait, data_capture_config, async_inference_config, serverless_inference_config, volume_size, model_data_download_timeout, container_startup_health_check_timeout, inference_recommendation_id, explainer_config, **kwargs)\u001b[0m\n\u001b[1;32m    311\u001b[0m             )\n\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 313\u001b[0;31m         return super(HuggingFaceModel, self).deploy(\n\u001b[0m\u001b[1;32m    314\u001b[0m             \u001b[0minitial_instance_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m             \u001b[0minstance_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/sagemaker/model.py\u001b[0m in \u001b[0;36mdeploy\u001b[0;34m(self, initial_instance_count, instance_type, serializer, deserializer, accelerator_type, endpoint_name, tags, kms_key, wait, data_capture_config, async_inference_config, serverless_inference_config, volume_size, model_data_download_timeout, container_startup_health_check_timeout, inference_recommendation_id, explainer_config, **kwargs)\u001b[0m\n\u001b[1;32m   1428\u001b[0m             \u001b[0mexplainer_config_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexplainer_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_to_request_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1430\u001b[0;31m         self.sagemaker_session.endpoint_from_production_variants(\n\u001b[0m\u001b[1;32m   1431\u001b[0m             \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendpoint_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1432\u001b[0m             \u001b[0mproduction_variants\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mproduction_variant\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36mendpoint_from_production_variants\u001b[0;34m(self, name, production_variants, tags, kms_key, wait, data_capture_config_dict, async_inference_config_dict, explainer_config_dict)\u001b[0m\n\u001b[1;32m   4725\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_endpoint_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4726\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4727\u001b[0;31m         return self.create_endpoint(\n\u001b[0m\u001b[1;32m   4728\u001b[0m             \u001b[0mendpoint_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mendpoint_tags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4729\u001b[0m         )\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36mcreate_endpoint\u001b[0;34m(self, endpoint_name, config_name, tags, wait)\u001b[0m\n\u001b[1;32m   4070\u001b[0m         )\n\u001b[1;32m   4071\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4072\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_for_endpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendpoint_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4073\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mendpoint_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4074\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36mwait_for_endpoint\u001b[0;34m(self, endpoint, poll)\u001b[0m\n\u001b[1;32m   4408\u001b[0m             \u001b[0mdict\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mReturn\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mthe\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDescribeEndpoint\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mAPI\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4409\u001b[0m         \"\"\"\n\u001b[0;32m-> 4410\u001b[0;31m         \u001b[0mdesc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_wait_until\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_deploy_done\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_client\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mendpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpoll\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4411\u001b[0m         \u001b[0mstatus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"EndpointStatus\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36m_wait_until\u001b[0;34m(callable_fn, poll)\u001b[0m\n\u001b[1;32m   6570\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6571\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mpoll\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6572\u001b[0;31m             \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6573\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcallable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6574\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mbotocore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mClientError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# deploy model to SageMaker Inference\n",
    "predictor = huggingface_estimator.deploy(\n",
    "\tinitial_instance_count=1, # number of instances\n",
    "\tinstance_type='ml.m5.xlarge' # ec2 instance type\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity': 'B-person',\n",
       "  'score': 0.7744121551513672,\n",
       "  'index': 4,\n",
       "  'word': 'ina',\n",
       "  'start': 11,\n",
       "  'end': 14},\n",
       " {'entity': 'B-person',\n",
       "  'score': 0.447575181722641,\n",
       "  'index': 5,\n",
       "  'word': '##am',\n",
       "  'start': 14,\n",
       "  'end': 16}]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.predict({\n",
    "\t\"inputs\": \"My name is inaam and I'm a solutions architect \",\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictor.delete_model()\n",
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFace\n",
    "\n",
    "# hyperparameters, which are passed into the training job\n",
    "hyperparameters={'epochs': 1,\n",
    "                 'train_batch_size': 32,\n",
    "                 'model_name':'distilbert-base-uncased'\n",
    "                 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "huggingface_estimator = HuggingFace(entry_point='train.py',\n",
    "                            source_dir='./scripts',\n",
    "                            instance_type='ml.p3.2xlarge',\n",
    "                            instance_count=1,\n",
    "                            role=role,\n",
    "                            transformers_version='4.26',\n",
    "                            pytorch_version='1.13',\n",
    "                            py_version='py39',\n",
    "                            hyperparameters = hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# starting the train job with our uploaded datasets as input\n",
    "huggingface_estimator.fit({'train': training_input_path, 'test': test_input_path})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploying the endpoint\n",
    "\n",
    "To deploy our endpoint, we call `deploy()` on our HuggingFace estimator object, passing in our desired number of instances and instance type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictor = huggingface_estimator.deploy(1, \"ml.g4dn.xlarge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we use the returned predictor object to call the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sentiment_input= {\"inputs\":\"I love using the new Inference DLC.\"}\n",
    "\n",
    "predictor.predict(sentiment_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we delete the endpoint again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictor.delete_model()\n",
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimator Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "container image used for training job: \n",
      "None\n",
      "\n",
      "s3 uri where the trained model is located: \n",
      "s3://sagemaker-us-east-1-151657023715/huggingface-pytorch-training-2023-08-22-14-31-55-436/output/model.tar.gz\n",
      "\n",
      "latest training job name for this estimator: \n",
      "huggingface-pytorch-training-2023-08-22-14-31-55-436\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# container image used for training job\n",
    "print(f\"container image used for training job: \\n{huggingface_estimator.image_uri}\\n\")\n",
    "\n",
    "# s3 uri where the trained model is located\n",
    "print(f\"s3 uri where the trained model is located: \\n{huggingface_estimator.model_data}\\n\")\n",
    "\n",
    "# latest training job name for this estimator\n",
    "print(f\"latest training job name for this estimator: \\n{huggingface_estimator.latest_training_job.name}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-08-22 14:39:15 Starting - Preparing the instances for training\n",
      "2023-08-22 14:39:15 Downloading - Downloading input data\n",
      "2023-08-22 14:39:15 Training - Training image download completed. Training in progress.\n",
      "2023-08-22 14:39:15 Uploading - Uploading generated training model\n",
      "2023-08-22 14:39:15 Completed - Training job completed\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2023-08-22 14:38:01,676 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2023-08-22 14:38:01,700 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-08-22 14:38:01,715 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2023-08-22 14:38:01,718 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2023-08-22 14:38:02,000 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.9 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: accelerate>=0.12.0 in /opt/conda/lib/python3.9/site-packages (from -r requirements.txt (line 1)) (0.16.0)\u001b[0m\n",
      "\u001b[34mCollecting seqeval\u001b[0m\n",
      "\u001b[34mDownloading seqeval-1.2.2.tar.gz (43 kB)\u001b[0m\n",
      "\u001b[34mââââââââââââââââââââââââââââââââââââââââ 43.6/43.6 kB 3.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: datasets>=1.8.0 in /opt/conda/lib/python3.9/site-packages (from -r requirements.txt (line 3)) (2.9.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch>=1.3 in /opt/conda/lib/python3.9/site-packages (from -r requirements.txt (line 4)) (1.13.1+cu117)\u001b[0m\n",
      "\u001b[34mCollecting evaluate\u001b[0m\n",
      "\u001b[34mDownloading evaluate-0.4.0-py3-none-any.whl (81 kB)\u001b[0m\n",
      "\u001b[34mââââââââââââââââââââââââââââââââââââââââ 81.4/81.4 kB 12.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: psutil in /opt/conda/lib/python3.9/site-packages (from accelerate>=0.12.0->-r requirements.txt (line 1)) (5.9.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.9/site-packages (from accelerate>=0.12.0->-r requirements.txt (line 1)) (23.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml in /opt/conda/lib/python3.9/site-packages (from accelerate>=0.12.0->-r requirements.txt (line 1)) (5.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.9/site-packages (from accelerate>=0.12.0->-r requirements.txt (line 1)) (1.23.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: scikit-learn>=0.21.3 in /opt/conda/lib/python3.9/site-packages (from seqeval->-r requirements.txt (line 2)) (1.2.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dill<0.3.7 in /opt/conda/lib/python3.9/site-packages (from datasets>=1.8.0->-r requirements.txt (line 3)) (0.3.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec[http]>=2021.11.1 in /opt/conda/lib/python3.9/site-packages (from datasets>=1.8.0->-r requirements.txt (line 3)) (2023.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.9/site-packages (from datasets>=1.8.0->-r requirements.txt (line 3)) (2.28.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.9/site-packages (from datasets>=1.8.0->-r requirements.txt (line 3)) (0.18.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.9/site-packages (from datasets>=1.8.0->-r requirements.txt (line 3)) (1.5.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow>=6.0.0 in /opt/conda/lib/python3.9/site-packages (from datasets>=1.8.0->-r requirements.txt (line 3)) (11.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in /opt/conda/lib/python3.9/site-packages (from datasets>=1.8.0->-r requirements.txt (line 3)) (0.12.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: xxhash in /opt/conda/lib/python3.9/site-packages (from datasets>=1.8.0->-r requirements.txt (line 3)) (3.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiohttp in /opt/conda/lib/python3.9/site-packages (from datasets>=1.8.0->-r requirements.txt (line 3)) (3.8.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.9/site-packages (from datasets>=1.8.0->-r requirements.txt (line 3)) (0.70.14)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.9/site-packages (from datasets>=1.8.0->-r requirements.txt (line 3)) (4.64.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.9/site-packages (from torch>=1.3->-r requirements.txt (line 4)) (4.4.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 3)) (1.3.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 3)) (1.3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 3)) (6.0.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 3)) (2.1.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 3)) (4.0.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 3)) (22.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 3)) (1.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.9/site-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets>=1.8.0->-r requirements.txt (line 3)) (3.9.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->datasets>=1.8.0->-r requirements.txt (line 3)) (3.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->datasets>=1.8.0->-r requirements.txt (line 3)) (2022.12.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->datasets>=1.8.0->-r requirements.txt (line 3)) (1.26.14)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.9/site-packages (from scikit-learn>=0.21.3->seqeval->-r requirements.txt (line 2)) (1.10.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.9/site-packages (from scikit-learn>=0.21.3->seqeval->-r requirements.txt (line 2)) (3.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.9/site-packages (from scikit-learn>=0.21.3->seqeval->-r requirements.txt (line 2)) (1.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.9/site-packages (from pandas->datasets>=1.8.0->-r requirements.txt (line 3)) (2.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.9/site-packages (from pandas->datasets>=1.8.0->-r requirements.txt (line 3)) (2022.7.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas->datasets>=1.8.0->-r requirements.txt (line 3)) (1.16.0)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: seqeval\u001b[0m\n",
      "\u001b[34mBuilding wheel for seqeval (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for seqeval (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16164 sha256=9371c19c7afdc660d05321de35781dd20d13a2b4eaa7602b17d0940b0310034e\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/e2/a5/92/2c80d1928733611c2747a9820e1324a6835524d9411510c142\u001b[0m\n",
      "\u001b[34mSuccessfully built seqeval\u001b[0m\n",
      "\u001b[34mInstalling collected packages: seqeval, evaluate\u001b[0m\n",
      "\u001b[34mSuccessfully installed evaluate-0.4.0 seqeval-1.2.2\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip is available: 23.0 -> 23.2.1\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m2023-08-22 14:38:09,016 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2023-08-22 14:38:09,016 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2023-08-22 14:38:09,063 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-08-22 14:38:09,104 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-08-22 14:38:09,145 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-08-22 14:38:09,160 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {},\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.p3.2xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"dataset_name\": \"wnut_17\",\n",
      "        \"do_eval\": \"true\",\n",
      "        \"do_train\": \"true\",\n",
      "        \"model_name_or_path\": \"distilbert-base-uncased\",\n",
      "        \"num_train_epochs\": 1,\n",
      "        \"output_dir\": \"/opt/ml/model\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {},\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.p3.2xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": true,\n",
      "    \"job_name\": \"huggingface-pytorch-training-2023-08-22-14-31-55-436\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-151657023715/huggingface-pytorch-training-2023-08-22-14-31-55-436/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"run_ner\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 1,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.p3.2xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.p3.2xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"run_ner.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"dataset_name\":\"wnut_17\",\"do_eval\":\"true\",\"do_train\":\"true\",\"model_name_or_path\":\"distilbert-base-uncased\",\"num_train_epochs\":1,\"output_dir\":\"/opt/ml/model\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=run_ner.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p3.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.2xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.p3.2xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.2xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=run_ner\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-151657023715/huggingface-pytorch-training-2023-08-22-14-31-55-436/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.p3.2xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"dataset_name\":\"wnut_17\",\"do_eval\":\"true\",\"do_train\":\"true\",\"model_name_or_path\":\"distilbert-base-uncased\",\"num_train_epochs\":1,\"output_dir\":\"/opt/ml/model\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.2xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":true,\"job_name\":\"huggingface-pytorch-training-2023-08-22-14-31-55-436\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-151657023715/huggingface-pytorch-training-2023-08-22-14-31-55-436/source/sourcedir.tar.gz\",\"module_name\":\"run_ner\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":1,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p3.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.2xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"run_ner.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--dataset_name\",\"wnut_17\",\"--do_eval\",\"true\",\"--do_train\",\"true\",\"--model_name_or_path\",\"distilbert-base-uncased\",\"--num_train_epochs\",\"1\",\"--output_dir\",\"/opt/ml/model\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_HP_DATASET_NAME=wnut_17\u001b[0m\n",
      "\u001b[34mSM_HP_DO_EVAL=true\u001b[0m\n",
      "\u001b[34mSM_HP_DO_TRAIN=true\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_NAME_OR_PATH=distilbert-base-uncased\u001b[0m\n",
      "\u001b[34mSM_HP_NUM_TRAIN_EPOCHS=1\u001b[0m\n",
      "\u001b[34mSM_HP_OUTPUT_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python39.zip:/opt/conda/lib/python3.9:/opt/conda/lib/python3.9/lib-dynload:/opt/conda/lib/python3.9/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.9 run_ner.py --dataset_name wnut_17 --do_eval true --do_train true --model_name_or_path distilbert-base-uncased --num_train_epochs 1 --output_dir /opt/ml/model\u001b[0m\n",
      "\u001b[34m[2023-08-22 14:38:11.437: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.26.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m2023-08-22 14:38:11,445 root         INFO     Using NamedTuple = typing._NamedTuple instead.\u001b[0m\n",
      "\u001b[34m2023-08-22 14:38:11,477 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[34m08/22/2023 14:38:15 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\u001b[0m\n",
      "\u001b[34m08/22/2023 14:38:15 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\u001b[0m\n",
      "\u001b[34m_n_gpu=1,\u001b[0m\n",
      "\u001b[34madafactor=False,\u001b[0m\n",
      "\u001b[34madam_beta1=0.9,\u001b[0m\n",
      "\u001b[34madam_beta2=0.999,\u001b[0m\n",
      "\u001b[34madam_epsilon=1e-08,\u001b[0m\n",
      "\u001b[34mauto_find_batch_size=False,\u001b[0m\n",
      "\u001b[34mbf16=False,\u001b[0m\n",
      "\u001b[34mbf16_full_eval=False,\u001b[0m\n",
      "\u001b[34mdata_seed=None,\u001b[0m\n",
      "\u001b[34mdataloader_drop_last=False,\u001b[0m\n",
      "\u001b[34mdataloader_num_workers=0,\u001b[0m\n",
      "\u001b[34mdataloader_pin_memory=True,\u001b[0m\n",
      "\u001b[34mddp_bucket_cap_mb=None,\u001b[0m\n",
      "\u001b[34mddp_find_unused_parameters=None,\u001b[0m\n",
      "\u001b[34mddp_timeout=1800,\u001b[0m\n",
      "\u001b[34mdebug=[],\u001b[0m\n",
      "\u001b[34mdeepspeed=None,\u001b[0m\n",
      "\u001b[34mdisable_tqdm=False,\u001b[0m\n",
      "\u001b[34mdo_eval=True,\u001b[0m\n",
      "\u001b[34mdo_predict=False,\u001b[0m\n",
      "\u001b[34mdo_train=True,\u001b[0m\n",
      "\u001b[34meval_accumulation_steps=None,\u001b[0m\n",
      "\u001b[34meval_delay=0,\u001b[0m\n",
      "\u001b[34meval_steps=None,\u001b[0m\n",
      "\u001b[34mevaluation_strategy=no,\u001b[0m\n",
      "\u001b[34mfp16=False,\u001b[0m\n",
      "\u001b[34mfp16_backend=auto,\u001b[0m\n",
      "\u001b[34mfp16_full_eval=False,\u001b[0m\n",
      "\u001b[34mfp16_opt_level=O1,\u001b[0m\n",
      "\u001b[34mfsdp=[],\u001b[0m\n",
      "\u001b[34mfsdp_min_num_params=0,\u001b[0m\n",
      "\u001b[34mfsdp_transformer_layer_cls_to_wrap=None,\u001b[0m\n",
      "\u001b[34mfull_determinism=False,\u001b[0m\n",
      "\u001b[34mgradient_accumulation_steps=1,\u001b[0m\n",
      "\u001b[34mgradient_checkpointing=False,\u001b[0m\n",
      "\u001b[34mgreater_is_better=None,\u001b[0m\n",
      "\u001b[34mgroup_by_length=False,\u001b[0m\n",
      "\u001b[34mhalf_precision_backend=auto,\u001b[0m\n",
      "\u001b[34mhub_model_id=None,\u001b[0m\n",
      "\u001b[34mhub_private_repo=False,\u001b[0m\n",
      "\u001b[34mhub_strategy=every_save,\u001b[0m\n",
      "\u001b[34mhub_token=<HUB_TOKEN>,\u001b[0m\n",
      "\u001b[34mignore_data_skip=False,\u001b[0m\n",
      "\u001b[34minclude_inputs_for_metrics=False,\u001b[0m\n",
      "\u001b[34mjit_mode_eval=False,\u001b[0m\n",
      "\u001b[34mlabel_names=None,\u001b[0m\n",
      "\u001b[34mlabel_smoothing_factor=0.0,\u001b[0m\n",
      "\u001b[34mlearning_rate=5e-05,\u001b[0m\n",
      "\u001b[34mlength_column_name=length,\u001b[0m\n",
      "\u001b[34mload_best_model_at_end=False,\u001b[0m\n",
      "\u001b[34mlocal_rank=-1,\u001b[0m\n",
      "\u001b[34mlog_level=passive,\u001b[0m\n",
      "\u001b[34mlog_level_replica=passive,\u001b[0m\n",
      "\u001b[34mlog_on_each_node=True,\u001b[0m\n",
      "\u001b[34mlogging_dir=/opt/ml/model/runs/Aug22_14-38-15_algo-1,\u001b[0m\n",
      "\u001b[34mlogging_first_step=False,\u001b[0m\n",
      "\u001b[34mlogging_nan_inf_filter=True,\u001b[0m\n",
      "\u001b[34mlogging_steps=500,\u001b[0m\n",
      "\u001b[34mlogging_strategy=steps,\u001b[0m\n",
      "\u001b[34mlr_scheduler_type=linear,\u001b[0m\n",
      "\u001b[34mmax_grad_norm=1.0,\u001b[0m\n",
      "\u001b[34mmax_steps=-1,\u001b[0m\n",
      "\u001b[34mmetric_for_best_model=None,\u001b[0m\n",
      "\u001b[34mmp_parameters=,\u001b[0m\n",
      "\u001b[34mno_cuda=False,\u001b[0m\n",
      "\u001b[34mnum_train_epochs=1.0,\u001b[0m\n",
      "\u001b[34moptim=adamw_hf,\u001b[0m\n",
      "\u001b[34moptim_args=None,\u001b[0m\n",
      "\u001b[34moutput_dir=/opt/ml/model,\u001b[0m\n",
      "\u001b[34moverwrite_output_dir=False,\u001b[0m\n",
      "\u001b[34mpast_index=-1,\u001b[0m\n",
      "\u001b[34mper_device_eval_batch_size=8,\u001b[0m\n",
      "\u001b[34mper_device_train_batch_size=8,\u001b[0m\n",
      "\u001b[34mprediction_loss_only=False,\u001b[0m\n",
      "\u001b[34mpush_to_hub=False,\u001b[0m\n",
      "\u001b[34mpush_to_hub_model_id=None,\u001b[0m\n",
      "\u001b[34mpush_to_hub_organization=None,\u001b[0m\n",
      "\u001b[34mpush_to_hub_token=<PUSH_TO_HUB_TOKEN>,\u001b[0m\n",
      "\u001b[34mray_scope=last,\u001b[0m\n",
      "\u001b[34mremove_unused_columns=True,\u001b[0m\n",
      "\u001b[34mreport_to=[],\u001b[0m\n",
      "\u001b[34mresume_from_checkpoint=None,\u001b[0m\n",
      "\u001b[34mrun_name=/opt/ml/model,\u001b[0m\n",
      "\u001b[34msave_on_each_node=False,\u001b[0m\n",
      "\u001b[34msave_steps=500,\u001b[0m\n",
      "\u001b[34msave_strategy=steps,\u001b[0m\n",
      "\u001b[34msave_total_limit=None,\u001b[0m\n",
      "\u001b[34mseed=42,\u001b[0m\n",
      "\u001b[34msharded_ddp=[],\u001b[0m\n",
      "\u001b[34mskip_memory_metrics=True,\u001b[0m\n",
      "\u001b[34mtf32=None,\u001b[0m\n",
      "\u001b[34mtorch_compile=False,\u001b[0m\n",
      "\u001b[34mtorch_compile_backend=None,\u001b[0m\n",
      "\u001b[34mtorch_compile_mode=None,\u001b[0m\n",
      "\u001b[34mtorchdynamo=None,\u001b[0m\n",
      "\u001b[34mtpu_metrics_debug=False,\u001b[0m\n",
      "\u001b[34mtpu_num_cores=None,\u001b[0m\n",
      "\u001b[34muse_ipex=False,\u001b[0m\n",
      "\u001b[34muse_legacy_prediction_loop=False,\u001b[0m\n",
      "\u001b[34muse_mps_device=False,\u001b[0m\n",
      "\u001b[34mwarmup_ratio=0.0,\u001b[0m\n",
      "\u001b[34mwarmup_steps=0,\u001b[0m\n",
      "\u001b[34mweight_decay=0.0,\u001b[0m\n",
      "\u001b[34mxpu_backend=None,\u001b[0m\n",
      "\u001b[34m)\u001b[0m\n",
      "\u001b[34m08/22/2023 14:38:15 - INFO - datasets.utils.file_utils - https://huggingface.co/datasets/wnut_17/resolve/main/wnut_17.py not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmp8wnh4phq\u001b[0m\n",
      "\u001b[34mDownloading builder script:   0%|          | 0.00/7.46k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading builder script: 100%|ââââââââââ| 7.46k/7.46k [00:00<00:00, 4.99MB/s]\u001b[0m\n",
      "\u001b[34m08/22/2023 14:38:15 - INFO - datasets.utils.file_utils - storing https://huggingface.co/datasets/wnut_17/resolve/main/wnut_17.py in cache at /root/.cache/huggingface/datasets/downloads/cb8d3189178ffd9a3e0220867b361cdf59515542cd5c1db0f6006f01b9535d68.b07bfe217645b44e29826533bdfe4cd6816f8d741f857a859e46518a167ade29.py\u001b[0m\n",
      "\u001b[34m08/22/2023 14:38:15 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/cb8d3189178ffd9a3e0220867b361cdf59515542cd5c1db0f6006f01b9535d68.b07bfe217645b44e29826533bdfe4cd6816f8d741f857a859e46518a167ade29.py\u001b[0m\n",
      "\u001b[34m08/22/2023 14:38:15 - INFO - datasets.utils.file_utils - https://huggingface.co/datasets/wnut_17/resolve/main/dataset_infos.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmppejezjzs\u001b[0m\n",
      "\u001b[34mDownloading metadata:   0%|          | 0.00/4.28k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading metadata: 100%|ââââââââââ| 4.28k/4.28k [00:00<00:00, 2.85MB/s]\u001b[0m\n",
      "\u001b[34m08/22/2023 14:38:15 - INFO - datasets.utils.file_utils - storing https://huggingface.co/datasets/wnut_17/resolve/main/dataset_infos.json in cache at /root/.cache/huggingface/datasets/downloads/9a25be179d8df8fde41e5baf597b8e9f5690a2316f4d2b06f8abe9b1d5359b33.c61b848cf9b3aba5b793ac61a125d5e65ccd3c5d4849f81c057e191df14e3d9b\u001b[0m\n",
      "\u001b[34m08/22/2023 14:38:15 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/9a25be179d8df8fde41e5baf597b8e9f5690a2316f4d2b06f8abe9b1d5359b33.c61b848cf9b3aba5b793ac61a125d5e65ccd3c5d4849f81c057e191df14e3d9b\u001b[0m\n",
      "\u001b[34m08/22/2023 14:38:15 - INFO - datasets.utils.file_utils - https://huggingface.co/datasets/wnut_17/resolve/main/README.md not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmprieazymr\u001b[0m\n",
      "\u001b[34mDownloading readme:   0%|          | 0.00/9.05k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading readme: 100%|ââââââââââ| 9.05k/9.05k [00:00<00:00, 6.79MB/s]\u001b[0m\n",
      "\u001b[34m08/22/2023 14:38:15 - INFO - datasets.utils.file_utils - storing https://huggingface.co/datasets/wnut_17/resolve/main/README.md in cache at /root/.cache/huggingface/datasets/downloads/6c71f17bfba45b9dd0381a6ef5fb312c34c7d039c23b3a9dc59bb2eb9c744a98.dba18617ae1987cc9dc3a85f4efa8f4926f7b4dbddce790a19f128730ad228a9\u001b[0m\n",
      "\u001b[34m08/22/2023 14:38:15 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/6c71f17bfba45b9dd0381a6ef5fb312c34c7d039c23b3a9dc59bb2eb9c744a98.dba18617ae1987cc9dc3a85f4efa8f4926f7b4dbddce790a19f128730ad228a9\u001b[0m\n",
      "\u001b[34m08/22/2023 14:38:15 - INFO - datasets.builder - No config specified, defaulting to the single config: wnut_17/wnut_17\u001b[0m\n",
      "\u001b[34m08/22/2023 14:38:15 - INFO - datasets.info - Loading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/wnut_17/077c7f08b8dbc800692e8c9186cdf3606d5849ab0e7be662e6135bb10eba54f9\u001b[0m\n",
      "\u001b[34m08/22/2023 14:38:16 - INFO - datasets.builder - Generating dataset wnut_17 (/root/.cache/huggingface/datasets/wnut_17/wnut_17/1.0.0/077c7f08b8dbc800692e8c9186cdf3606d5849ab0e7be662e6135bb10eba54f9)\u001b[0m\n",
      "\u001b[34mDownloading and preparing dataset wnut_17/wnut_17 to /root/.cache/huggingface/datasets/wnut_17/wnut_17/1.0.0/077c7f08b8dbc800692e8c9186cdf3606d5849ab0e7be662e6135bb10eba54f9...\u001b[0m\n",
      "\u001b[34m08/22/2023 14:38:16 - INFO - datasets.builder - Dataset not on Hf google storage. Downloading and preparing it from source\u001b[0m\n",
      "\u001b[34mDownloading data files:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m08/22/2023 14:38:16 - INFO - datasets.utils.file_utils - https://raw.githubusercontent.com/leondz/emerging_entities_17/master/wnut17train.conll not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmpqgg5ftm2\u001b[0m\n",
      "\u001b[34mDownloading data:   0%|          | 0.00/185k [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading data: 494kB [00:00, 10.4MB/s]\u001b[0m\n",
      "\u001b[34m08/22/2023 14:38:16 - INFO - datasets.utils.file_utils - storing https://raw.githubusercontent.com/leondz/emerging_entities_17/master/wnut17train.conll in cache at /root/.cache/huggingface/datasets/downloads/1c663116ec11ffb0a6f2518c6846086c9e62916c887d0cff71fff8933111533f\u001b[0m\n",
      "\u001b[34m08/22/2023 14:38:16 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/1c663116ec11ffb0a6f2518c6846086c9e62916c887d0cff71fff8933111533f\u001b[0m\n",
      "\u001b[34mDownloading data files:  33%|ââââ      | 1/3 [00:00<00:00,  2.17it/s]\u001b[0m\n",
      "\u001b[34m08/22/2023 14:38:16 - INFO - datasets.utils.file_utils - https://raw.githubusercontent.com/leondz/emerging_entities_17/master/emerging.dev.conll not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmptygb8xxj\u001b[0m\n",
      "\u001b[34mDownloading data:   0%|          | 0.00/39.1k [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading data: 115kB [00:00, 8.52MB/s]\u001b[0m\n",
      "\u001b[34m08/22/2023 14:38:16 - INFO - datasets.utils.file_utils - storing https://raw.githubusercontent.com/leondz/emerging_entities_17/master/emerging.dev.conll in cache at /root/.cache/huggingface/datasets/downloads/8bf465467e300aa565a240a1c61a226ed3f4b05bf675f8b27c0f51cce5a3fb79\u001b[0m\n",
      "\u001b[34m08/22/2023 14:38:16 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/8bf465467e300aa565a240a1c61a226ed3f4b05bf675f8b27c0f51cce5a3fb79\u001b[0m\n",
      "\u001b[34mDownloading data files:  67%|âââââââ   | 2/3 [00:00<00:00,  3.43it/s]\u001b[0m\n",
      "\u001b[34m08/22/2023 14:38:16 - INFO - datasets.utils.file_utils - https://raw.githubusercontent.com/leondz/emerging_entities_17/master/emerging.test.annotated not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmp_8al2iq1\u001b[0m\n",
      "\u001b[34mDownloading data:   0%|          | 0.00/66.9k [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading data: 192kB [00:00, 5.43MB/s]\u001b[0m\n",
      "\u001b[34m08/22/2023 14:38:16 - INFO - datasets.utils.file_utils - storing https://raw.githubusercontent.com/leondz/emerging_entities_17/master/emerging.test.annotated in cache at /root/.cache/huggingface/datasets/downloads/44b136748a9f7a18a25ffe3556a6d0ece455872c29794ab3fd9eb692012f86b0\u001b[0m\n",
      "\u001b[34m08/22/2023 14:38:16 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/44b136748a9f7a18a25ffe3556a6d0ece455872c29794ab3fd9eb692012f86b0\u001b[0m\n",
      "\u001b[34mDownloading data files: 100%|ââââââââââ| 3/3 [00:00<00:00,  3.72it/s]\u001b[0m\n",
      "\u001b[34mDownloading data files: 100%|ââââââââââ| 3/3 [00:00<00:00,  3.43it/s]\u001b[0m\n",
      "\u001b[34m08/22/2023 14:38:16 - INFO - datasets.download.download_manager - Downloading took 0.0 min\u001b[0m\n",
      "\u001b[34m08/22/2023 14:38:16 - INFO - datasets.download.download_manager - Checksum Computation took 0.0 min\u001b[0m\n",
      "\u001b[34mExtracting data files:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mExtracting data files: 100%|ââââââââââ| 3/3 [00:00<00:00, 1669.26it/s]\u001b[0m\n",
      "\u001b[34m08/22/2023 14:38:16 - INFO - datasets.utils.info_utils - Unable to verify checksums.\u001b[0m\n",
      "\u001b[34m08/22/2023 14:38:16 - INFO - datasets.builder - Generating train split\u001b[0m\n",
      "\u001b[34mGenerating train split:   0%|          | 0/3394 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split:  21%|ââ        | 708/3394 [00:00<00:00, 7048.06 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split:  50%|âââââ     | 1708/3394 [00:00<00:00, 4673.34 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split:  71%|ââââââââ  | 2420/3394 [00:00<00:00, 5427.29 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split:  94%|ââââââââââ| 3198/3394 [00:00<00:00, 6144.41 examples/s]\u001b[0m\n",
      "\u001b[34m08/22/2023 14:38:17 - INFO - datasets.builder - Generating validation split\u001b[0m\n",
      "\u001b[34mGenerating validation split:   0%|          | 0/1009 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mGenerating validation split:  86%|âââââââââ | 870/1009 [00:00<00:00, 8665.59 examples/s]\u001b[0m\n",
      "\u001b[34m08/22/2023 14:38:17 - INFO - datasets.builder - Generating test split\u001b[0m\n",
      "\u001b[34mGenerating test split:   0%|          | 0/1287 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mGenerating test split:  59%|ââââââ    | 758/1287 [00:00<00:00, 7547.65 examples/s]\u001b[0m\n",
      "\u001b[34m08/22/2023 14:38:17 - INFO - datasets.utils.info_utils - All the splits matched successfully.\u001b[0m\n",
      "\u001b[34mDataset wnut_17 downloaded and prepared to /root/.cache/huggingface/datasets/wnut_17/wnut_17/1.0.0/077c7f08b8dbc800692e8c9186cdf3606d5849ab0e7be662e6135bb10eba54f9. Subsequent calls will reuse this data.\u001b[0m\n",
      "\u001b[34m0%|          | 0/3 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m100%|ââââââââââ| 3/3 [00:00<00:00, 673.24it/s]\u001b[0m\n",
      "\u001b[34mDownloading (â¦)lve/main/config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading (â¦)lve/main/config.json: 100%|ââââââââââ| 483/483 [00:00<00:00, 58.3kB/s]\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:660] 2023-08-22 14:38:17,993 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/6cdc0aad91f5ae2e6712e91bc7b65d1cf5c05411/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:660] 2023-08-22 14:38:17,993 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/6cdc0aad91f5ae2e6712e91bc7b65d1cf5c05411/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:712] 2023-08-22 14:38:17,997 >> Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"finetuning_task\": \"ner\",\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\",\n",
      "    \"9\": \"LABEL_9\",\n",
      "    \"10\": \"LABEL_10\",\n",
      "    \"11\": \"LABEL_11\",\n",
      "    \"12\": \"LABEL_12\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_10\": 10,\n",
      "    \"LABEL_11\": 11,\n",
      "    \"LABEL_12\": 12,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8,\n",
      "    \"LABEL_9\": 9\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.26.0\",\n",
      "  \"vocab_size\": 30522\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:712] 2023-08-22 14:38:17,997 >> Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"finetuning_task\": \"ner\",\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\",\n",
      "    \"9\": \"LABEL_9\",\n",
      "    \"10\": \"LABEL_10\",\n",
      "    \"11\": \"LABEL_11\",\n",
      "    \"12\": \"LABEL_12\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_10\": 10,\n",
      "    \"LABEL_11\": 11,\n",
      "    \"LABEL_12\": 12,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8,\n",
      "    \"LABEL_9\": 9\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.26.0\",\n",
      "  \"vocab_size\": 30522\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mDownloading (â¦)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading (â¦)okenizer_config.json: 100%|ââââââââââ| 28.0/28.0 [00:00<00:00, 3.66kB/s]\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:660] 2023-08-22 14:38:18,120 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/6cdc0aad91f5ae2e6712e91bc7b65d1cf5c05411/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:660] 2023-08-22 14:38:18,120 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/6cdc0aad91f5ae2e6712e91bc7b65d1cf5c05411/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:712] 2023-08-22 14:38:18,121 >> Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.26.0\",\n",
      "  \"vocab_size\": 30522\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:712] 2023-08-22 14:38:18,121 >> Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.26.0\",\n",
      "  \"vocab_size\": 30522\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mDownloading (â¦)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading (â¦)solve/main/vocab.txt: 100%|ââââââââââ| 232k/232k [00:00<00:00, 37.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading (â¦)/main/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading (â¦)/main/tokenizer.json: 100%|ââââââââââ| 466k/466k [00:00<00:00, 74.9MB/s]\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1802] 2023-08-22 14:38:18,485 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/6cdc0aad91f5ae2e6712e91bc7b65d1cf5c05411/vocab.txt\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1802] 2023-08-22 14:38:18,485 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/6cdc0aad91f5ae2e6712e91bc7b65d1cf5c05411/tokenizer.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1802] 2023-08-22 14:38:18,485 >> loading file added_tokens.json from cache at None\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1802] 2023-08-22 14:38:18,485 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/6cdc0aad91f5ae2e6712e91bc7b65d1cf5c05411/vocab.txt\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1802] 2023-08-22 14:38:18,485 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/6cdc0aad91f5ae2e6712e91bc7b65d1cf5c05411/tokenizer.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1802] 2023-08-22 14:38:18,485 >> loading file added_tokens.json from cache at None\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1802] 2023-08-22 14:38:18,485 >> loading file special_tokens_map.json from cache at None\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1802] 2023-08-22 14:38:18,485 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/6cdc0aad91f5ae2e6712e91bc7b65d1cf5c05411/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1802] 2023-08-22 14:38:18,485 >> loading file special_tokens_map.json from cache at None\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1802] 2023-08-22 14:38:18,485 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/6cdc0aad91f5ae2e6712e91bc7b65d1cf5c05411/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:660] 2023-08-22 14:38:18,486 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/6cdc0aad91f5ae2e6712e91bc7b65d1cf5c05411/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:660] 2023-08-22 14:38:18,486 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/6cdc0aad91f5ae2e6712e91bc7b65d1cf5c05411/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:712] 2023-08-22 14:38:18,486 >> Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.26.0\",\n",
      "  \"vocab_size\": 30522\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:712] 2023-08-22 14:38:18,486 >> Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.26.0\",\n",
      "  \"vocab_size\": 30522\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mDownloading (â¦)\"pytorch_model.bin\";:   0%|          | 0.00/268M [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading (â¦)\"pytorch_model.bin\";:  16%|ââ        | 41.9M/268M [00:00<00:00, 378MB/s]\u001b[0m\n",
      "\u001b[34mDownloading (â¦)\"pytorch_model.bin\";:  35%|ââââ      | 94.4M/268M [00:00<00:00, 409MB/s]\u001b[0m\n",
      "\u001b[34mDownloading (â¦)\"pytorch_model.bin\";:  55%|ââââââ    | 147M/268M [00:00<00:00, 432MB/s]\u001b[0m\n",
      "\u001b[34mDownloading (â¦)\"pytorch_model.bin\";:  74%|ââââââââ  | 199M/268M [00:00<00:00, 444MB/s]\u001b[0m\n",
      "\u001b[34mDownloading (â¦)\"pytorch_model.bin\";:  94%|ââââââââââ| 252M/268M [00:00<00:00, 450MB/s]\u001b[0m\n",
      "\u001b[34mDownloading (â¦)\"pytorch_model.bin\";: 100%|ââââââââââ| 268M/268M [00:00<00:00, 434MB/s]\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:2275] 2023-08-22 14:38:19,277 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/6cdc0aad91f5ae2e6712e91bc7b65d1cf5c05411/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:2275] 2023-08-22 14:38:19,277 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/6cdc0aad91f5ae2e6712e91bc7b65d1cf5c05411/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[WARNING|modeling_utils.py:2847] 2023-08-22 14:38:20,148 >> Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForTokenClassification: ['vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_transform.bias']\u001b[0m\n",
      "\u001b[34m- This IS expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m- This IS NOT expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[WARNING|modeling_utils.py:2859] 2023-08-22 14:38:20,148 >> Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[WARNING|modeling_utils.py:2847] 2023-08-22 14:38:20,148 >> Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForTokenClassification: ['vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_transform.bias']\u001b[0m\n",
      "\u001b[34m- This IS expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m- This IS NOT expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[WARNING|modeling_utils.py:2859] 2023-08-22 14:38:20,148 >> Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   0%|          | 0/4 [00:00<?, ?ba/s]\u001b[0m\n",
      "\u001b[34m08/22/2023 14:38:20 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/wnut_17/wnut_17/1.0.0/077c7f08b8dbc800692e8c9186cdf3606d5849ab0e7be662e6135bb10eba54f9/cache-9fe3fef6ef62b171.arrow\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  25%|âââ       | 1/4 [00:00<00:00,  9.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:  75%|ââââââââ  | 3/4 [00:00<00:00, 10.02ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset: 100%|ââââââââââ| 4/4 [00:00<00:00, 11.70ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on validation dataset:   0%|          | 0/2 [00:00<?, ?ba/s]\u001b[0m\n",
      "\u001b[34m08/22/2023 14:38:20 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/wnut_17/wnut_17/1.0.0/077c7f08b8dbc800692e8c9186cdf3606d5849ab0e7be662e6135bb10eba54f9/cache-ed293169f9c446f8.arrow\u001b[0m\n",
      "\u001b[34mRunning tokenizer on validation dataset: 100%|ââââââââââ| 2/2 [00:00<00:00, 24.42ba/s]\u001b[0m\n",
      "\u001b[34mDownloading builder script:   0%|          | 0.00/6.34k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading builder script: 100%|ââââââââââ| 6.34k/6.34k [00:00<00:00, 5.42MB/s]\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:710] 2023-08-22 14:38:22,334 >> The following columns in the training set don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: ner_tags, tokens, id. If ner_tags, tokens, id are not expected by `DistilBertForTokenClassification.forward`,  you can safely ignore this message.\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:710] 2023-08-22 14:38:22,334 >> The following columns in the training set don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: ner_tags, tokens, id. If ner_tags, tokens, id are not expected by `DistilBertForTokenClassification.forward`,  you can safely ignore this message.\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1650] 2023-08-22 14:38:22,344 >> ***** Running training *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1650] 2023-08-22 14:38:22,344 >> ***** Running training *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1651] 2023-08-22 14:38:22,344 >>   Num examples = 3394\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1652] 2023-08-22 14:38:22,344 >>   Num Epochs = 1\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1651] 2023-08-22 14:38:22,344 >>   Num examples = 3394\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1652] 2023-08-22 14:38:22,344 >>   Num Epochs = 1\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1653] 2023-08-22 14:38:22,345 >>   Instantaneous batch size per device = 8\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1654] 2023-08-22 14:38:22,345 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1653] 2023-08-22 14:38:22,345 >>   Instantaneous batch size per device = 8\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1654] 2023-08-22 14:38:22,345 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1655] 2023-08-22 14:38:22,345 >>   Gradient Accumulation steps = 1\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1656] 2023-08-22 14:38:22,345 >>   Total optimization steps = 425\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1655] 2023-08-22 14:38:22,345 >>   Gradient Accumulation steps = 1\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1656] 2023-08-22 14:38:22,345 >>   Total optimization steps = 425\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1657] 2023-08-22 14:38:22,345 >>   Number of trainable parameters = 66372877\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1657] 2023-08-22 14:38:22,345 >>   Number of trainable parameters = 66372877\u001b[0m\n",
      "\u001b[34m0%|          | 0/425 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[2023-08-22 14:38:22.460: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.26.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[2023-08-22 14:38:22.503 algo-1:68 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2023-08-22 14:38:22.545 algo-1:68 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2023-08-22 14:38:22.546 algo-1:68 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2023-08-22 14:38:22.546 algo-1:68 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2023-08-22 14:38:22.547 algo-1:68 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2023-08-22 14:38:22.547 algo-1:68 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[WARNING|logging.py:281] 2023-08-22 14:38:22,550 >> You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34m[WARNING|logging.py:281] 2023-08-22 14:38:22,550 >> You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34m0%|          | 1/425 [00:01<09:10,  1.30s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 4/425 [00:01<01:58,  3.55it/s]\u001b[0m\n",
      "\u001b[34m2%|â         | 7/425 [00:01<01:04,  6.45it/s]\u001b[0m\n",
      "\u001b[34m2%|â         | 10/425 [00:01<00:44,  9.31it/s]\u001b[0m\n",
      "\u001b[34m3%|â         | 13/425 [00:01<00:33, 12.28it/s]\u001b[0m\n",
      "\u001b[34m4%|â         | 16/425 [00:01<00:27, 15.10it/s]\u001b[0m\n",
      "\u001b[34m4%|â         | 19/425 [00:02<00:23, 17.49it/s]\u001b[0m\n",
      "\u001b[34m5%|â         | 22/425 [00:02<00:20, 19.34it/s]\u001b[0m\n",
      "\u001b[34m6%|â         | 25/425 [00:02<00:19, 20.36it/s]\u001b[0m\n",
      "\u001b[34m7%|â         | 28/425 [00:02<00:18, 21.06it/s]\u001b[0m\n",
      "\u001b[34m7%|â         | 31/425 [00:02<00:17, 21.95it/s]\u001b[0m\n",
      "\u001b[34m8%|â         | 34/425 [00:02<00:17, 22.48it/s]\u001b[0m\n",
      "\u001b[34m9%|â         | 37/425 [00:02<00:16, 22.98it/s]\u001b[0m\n",
      "\u001b[34m9%|â         | 40/425 [00:02<00:16, 23.78it/s]\u001b[0m\n",
      "\u001b[34m10%|â         | 43/425 [00:03<00:15, 24.49it/s]\u001b[0m\n",
      "\u001b[34m11%|â         | 46/425 [00:03<00:15, 25.11it/s]\u001b[0m\n",
      "\u001b[34m12%|ââ        | 49/425 [00:03<00:14, 25.42it/s]\u001b[0m\n",
      "\u001b[34m12%|ââ        | 52/425 [00:03<00:14, 25.46it/s]\u001b[0m\n",
      "\u001b[34m13%|ââ        | 55/425 [00:03<00:14, 25.33it/s]\u001b[0m\n",
      "\u001b[34m14%|ââ        | 58/425 [00:03<00:14, 24.91it/s]\u001b[0m\n",
      "\u001b[34m14%|ââ        | 61/425 [00:03<00:14, 25.54it/s]\u001b[0m\n",
      "\u001b[34m15%|ââ        | 64/425 [00:03<00:14, 25.54it/s]\u001b[0m\n",
      "\u001b[34m16%|ââ        | 67/425 [00:03<00:14, 25.32it/s]\u001b[0m\n",
      "\u001b[34m16%|ââ        | 70/425 [00:04<00:14, 24.73it/s]\u001b[0m\n",
      "\u001b[34m17%|ââ        | 73/425 [00:04<00:14, 24.73it/s]\u001b[0m\n",
      "\u001b[34m18%|ââ        | 76/425 [00:04<00:13, 25.14it/s]\u001b[0m\n",
      "\u001b[34m19%|ââ        | 79/425 [00:04<00:13, 25.20it/s]\u001b[0m\n",
      "\u001b[34m19%|ââ        | 82/425 [00:04<00:13, 24.60it/s]\u001b[0m\n",
      "\u001b[34m20%|ââ        | 85/425 [00:04<00:13, 24.77it/s]\u001b[0m\n",
      "\u001b[34m21%|ââ        | 88/425 [00:04<00:13, 24.46it/s]\u001b[0m\n",
      "\u001b[34m21%|âââ       | 91/425 [00:04<00:13, 24.26it/s]\u001b[0m\n",
      "\u001b[34m22%|âââ       | 94/425 [00:05<00:13, 24.62it/s]\u001b[0m\n",
      "\u001b[34m23%|âââ       | 97/425 [00:05<00:13, 24.68it/s]\u001b[0m\n",
      "\u001b[34m24%|âââ       | 100/425 [00:05<00:13, 24.52it/s]\u001b[0m\n",
      "\u001b[34m24%|âââ       | 103/425 [00:05<00:13, 24.39it/s]\u001b[0m\n",
      "\u001b[34m25%|âââ       | 106/425 [00:05<00:13, 24.44it/s]\u001b[0m\n",
      "\u001b[34m26%|âââ       | 109/425 [00:05<00:12, 25.01it/s]\u001b[0m\n",
      "\u001b[34m26%|âââ       | 112/425 [00:05<00:12, 25.30it/s]\u001b[0m\n",
      "\u001b[34m27%|âââ       | 115/425 [00:05<00:12, 25.52it/s]\u001b[0m\n",
      "\u001b[34m28%|âââ       | 118/425 [00:06<00:11, 25.69it/s]\u001b[0m\n",
      "\u001b[34m28%|âââ       | 121/425 [00:06<00:11, 25.91it/s]\u001b[0m\n",
      "\u001b[34m29%|âââ       | 124/425 [00:06<00:11, 25.83it/s]\u001b[0m\n",
      "\u001b[34m30%|âââ       | 127/425 [00:06<00:11, 25.81it/s]\u001b[0m\n",
      "\u001b[34m31%|âââ       | 130/425 [00:06<00:11, 25.81it/s]\u001b[0m\n",
      "\u001b[34m31%|ââââ      | 133/425 [00:06<00:11, 25.95it/s]\u001b[0m\n",
      "\u001b[34m32%|ââââ      | 136/425 [00:06<00:11, 26.09it/s]\u001b[0m\n",
      "\u001b[34m33%|ââââ      | 139/425 [00:06<00:10, 26.23it/s]\u001b[0m\n",
      "\u001b[34m33%|ââââ      | 142/425 [00:06<00:10, 26.43it/s]\u001b[0m\n",
      "\u001b[34m34%|ââââ      | 145/425 [00:07<00:10, 25.70it/s]\u001b[0m\n",
      "\u001b[34m35%|ââââ      | 148/425 [00:07<00:10, 25.50it/s]\u001b[0m\n",
      "\u001b[34m36%|ââââ      | 151/425 [00:07<00:10, 24.93it/s]\u001b[0m\n",
      "\u001b[34m36%|ââââ      | 154/425 [00:07<00:10, 25.38it/s]\u001b[0m\n",
      "\u001b[34m37%|ââââ      | 157/425 [00:07<00:10, 25.41it/s]\u001b[0m\n",
      "\u001b[34m38%|ââââ      | 160/425 [00:07<00:10, 24.72it/s]\u001b[0m\n",
      "\u001b[34m38%|ââââ      | 163/425 [00:07<00:10, 24.59it/s]\u001b[0m\n",
      "\u001b[34m39%|ââââ      | 166/425 [00:07<00:10, 24.55it/s]\u001b[0m\n",
      "\u001b[34m40%|ââââ      | 169/425 [00:08<00:10, 24.42it/s]\u001b[0m\n",
      "\u001b[34m40%|ââââ      | 172/425 [00:08<00:10, 24.15it/s]\u001b[0m\n",
      "\u001b[34m41%|ââââ      | 175/425 [00:08<00:10, 24.39it/s]\u001b[0m\n",
      "\u001b[34m42%|âââââ     | 178/425 [00:08<00:10, 24.35it/s]\u001b[0m\n",
      "\u001b[34m43%|âââââ     | 181/425 [00:08<00:09, 24.61it/s]\u001b[0m\n",
      "\u001b[34m43%|âââââ     | 184/425 [00:08<00:09, 24.18it/s]\u001b[0m\n",
      "\u001b[34m44%|âââââ     | 187/425 [00:08<00:09, 23.82it/s]\u001b[0m\n",
      "\u001b[34m45%|âââââ     | 190/425 [00:08<00:10, 23.26it/s]\u001b[0m\n",
      "\u001b[34m45%|âââââ     | 193/425 [00:09<00:09, 23.35it/s]\u001b[0m\n",
      "\u001b[34m46%|âââââ     | 196/425 [00:09<00:09, 23.11it/s]\u001b[0m\n",
      "\u001b[34m47%|âââââ     | 199/425 [00:09<00:09, 23.26it/s]\u001b[0m\n",
      "\u001b[34m48%|âââââ     | 202/425 [00:09<00:09, 23.53it/s]\u001b[0m\n",
      "\u001b[34m48%|âââââ     | 205/425 [00:09<00:09, 24.03it/s]\u001b[0m\n",
      "\u001b[34m49%|âââââ     | 208/425 [00:09<00:08, 24.63it/s]\u001b[0m\n",
      "\u001b[34m50%|âââââ     | 211/425 [00:09<00:08, 24.65it/s]\u001b[0m\n",
      "\u001b[34m50%|âââââ     | 214/425 [00:09<00:08, 24.96it/s]\u001b[0m\n",
      "\u001b[34m51%|âââââ     | 217/425 [00:10<00:08, 24.49it/s]\u001b[0m\n",
      "\u001b[34m52%|ââââââ    | 220/425 [00:10<00:08, 24.23it/s]\u001b[0m\n",
      "\u001b[34m52%|ââââââ    | 223/425 [00:10<00:08, 23.98it/s]\u001b[0m\n",
      "\u001b[34m53%|ââââââ    | 226/425 [00:10<00:08, 24.14it/s]\u001b[0m\n",
      "\u001b[34m54%|ââââââ    | 229/425 [00:10<00:08, 24.08it/s]\u001b[0m\n",
      "\u001b[34m55%|ââââââ    | 232/425 [00:10<00:08, 23.95it/s]\u001b[0m\n",
      "\u001b[34m55%|ââââââ    | 235/425 [00:10<00:07, 24.33it/s]\u001b[0m\n",
      "\u001b[34m56%|ââââââ    | 238/425 [00:10<00:07, 24.97it/s]\u001b[0m\n",
      "\u001b[34m57%|ââââââ    | 241/425 [00:11<00:07, 25.36it/s]\u001b[0m\n",
      "\u001b[34m57%|ââââââ    | 244/425 [00:11<00:07, 25.75it/s]\u001b[0m\n",
      "\u001b[34m58%|ââââââ    | 247/425 [00:11<00:06, 26.00it/s]\u001b[0m\n",
      "\u001b[34m59%|ââââââ    | 250/425 [00:11<00:06, 26.05it/s]\u001b[0m\n",
      "\u001b[34m60%|ââââââ    | 253/425 [00:11<00:06, 25.67it/s]\u001b[0m\n",
      "\u001b[34m60%|ââââââ    | 256/425 [00:11<00:06, 25.79it/s]\u001b[0m\n",
      "\u001b[34m61%|ââââââ    | 259/425 [00:11<00:06, 25.82it/s]\u001b[0m\n",
      "\u001b[34m62%|âââââââ   | 262/425 [00:11<00:06, 25.95it/s]\u001b[0m\n",
      "\u001b[34m62%|âââââââ   | 265/425 [00:11<00:06, 26.02it/s]\u001b[0m\n",
      "\u001b[34m63%|âââââââ   | 268/425 [00:12<00:06, 25.97it/s]\u001b[0m\n",
      "\u001b[34m64%|âââââââ   | 271/425 [00:12<00:05, 25.92it/s]\u001b[0m\n",
      "\u001b[34m64%|âââââââ   | 274/425 [00:12<00:05, 25.53it/s]\u001b[0m\n",
      "\u001b[34m65%|âââââââ   | 277/425 [00:12<00:05, 25.08it/s]\u001b[0m\n",
      "\u001b[34m66%|âââââââ   | 280/425 [00:12<00:05, 24.93it/s]\u001b[0m\n",
      "\u001b[34m67%|âââââââ   | 283/425 [00:12<00:05, 25.20it/s]\u001b[0m\n",
      "\u001b[34m67%|âââââââ   | 286/425 [00:12<00:05, 25.50it/s]\u001b[0m\n",
      "\u001b[34m68%|âââââââ   | 289/425 [00:12<00:05, 25.87it/s]\u001b[0m\n",
      "\u001b[34m69%|âââââââ   | 292/425 [00:12<00:05, 26.21it/s]\u001b[0m\n",
      "\u001b[34m69%|âââââââ   | 295/425 [00:13<00:04, 26.48it/s]\u001b[0m\n",
      "\u001b[34m70%|âââââââ   | 298/425 [00:13<00:04, 26.05it/s]\u001b[0m\n",
      "\u001b[34m71%|âââââââ   | 301/425 [00:13<00:04, 25.81it/s]\u001b[0m\n",
      "\u001b[34m72%|ââââââââ  | 304/425 [00:13<00:04, 25.76it/s]\u001b[0m\n",
      "\u001b[34m72%|ââââââââ  | 307/425 [00:13<00:04, 25.60it/s]\u001b[0m\n",
      "\u001b[34m73%|ââââââââ  | 310/425 [00:13<00:04, 25.31it/s]\u001b[0m\n",
      "\u001b[34m74%|ââââââââ  | 313/425 [00:13<00:04, 25.25it/s]\u001b[0m\n",
      "\u001b[34m74%|ââââââââ  | 316/425 [00:13<00:04, 25.08it/s]\u001b[0m\n",
      "\u001b[34m75%|ââââââââ  | 319/425 [00:14<00:04, 25.26it/s]\u001b[0m\n",
      "\u001b[34m76%|ââââââââ  | 322/425 [00:14<00:04, 25.72it/s]\u001b[0m\n",
      "\u001b[34m76%|ââââââââ  | 325/425 [00:14<00:03, 25.95it/s]\u001b[0m\n",
      "\u001b[34m77%|ââââââââ  | 328/425 [00:14<00:03, 25.88it/s]\u001b[0m\n",
      "\u001b[34m78%|ââââââââ  | 331/425 [00:14<00:03, 25.58it/s]\u001b[0m\n",
      "\u001b[34m79%|ââââââââ  | 334/425 [00:14<00:03, 26.10it/s]\u001b[0m\n",
      "\u001b[34m79%|ââââââââ  | 337/425 [00:14<00:03, 25.47it/s]\u001b[0m\n",
      "\u001b[34m80%|ââââââââ  | 340/425 [00:14<00:03, 25.65it/s]\u001b[0m\n",
      "\u001b[34m81%|ââââââââ  | 343/425 [00:14<00:03, 25.49it/s]\u001b[0m\n",
      "\u001b[34m81%|âââââââââ | 346/425 [00:15<00:03, 25.19it/s]\u001b[0m\n",
      "\u001b[34m82%|âââââââââ | 349/425 [00:15<00:03, 25.05it/s]\u001b[0m\n",
      "\u001b[34m83%|âââââââââ | 352/425 [00:15<00:02, 25.44it/s]\u001b[0m\n",
      "\u001b[34m84%|âââââââââ | 355/425 [00:15<00:02, 25.20it/s]\u001b[0m\n",
      "\u001b[34m84%|âââââââââ | 358/425 [00:15<00:02, 25.32it/s]\u001b[0m\n",
      "\u001b[34m85%|âââââââââ | 361/425 [00:15<00:02, 25.24it/s]\u001b[0m\n",
      "\u001b[34m86%|âââââââââ | 364/425 [00:15<00:02, 24.63it/s]\u001b[0m\n",
      "\u001b[34m86%|âââââââââ | 367/425 [00:15<00:02, 24.22it/s]\u001b[0m\n",
      "\u001b[34m87%|âââââââââ | 370/425 [00:16<00:02, 23.86it/s]\u001b[0m\n",
      "\u001b[34m88%|âââââââââ | 373/425 [00:16<00:02, 23.79it/s]\u001b[0m\n",
      "\u001b[34m88%|âââââââââ | 376/425 [00:16<00:02, 24.39it/s]\u001b[0m\n",
      "\u001b[34m89%|âââââââââ | 379/425 [00:16<00:01, 24.76it/s]\u001b[0m\n",
      "\u001b[34m90%|âââââââââ | 382/425 [00:16<00:01, 25.03it/s]\u001b[0m\n",
      "\u001b[34m91%|âââââââââ | 385/425 [00:16<00:01, 25.04it/s]\u001b[0m\n",
      "\u001b[34m91%|ââââââââââ| 388/425 [00:16<00:01, 25.22it/s]\u001b[0m\n",
      "\u001b[34m92%|ââââââââââ| 391/425 [00:16<00:01, 25.28it/s]\u001b[0m\n",
      "\u001b[34m93%|ââââââââââ| 394/425 [00:17<00:01, 25.37it/s]\u001b[0m\n",
      "\u001b[34m93%|ââââââââââ| 397/425 [00:17<00:01, 25.17it/s]\u001b[0m\n",
      "\u001b[34m94%|ââââââââââ| 400/425 [00:17<00:01, 24.55it/s]\u001b[0m\n",
      "\u001b[34m95%|ââââââââââ| 403/425 [00:17<00:00, 24.08it/s]\u001b[0m\n",
      "\u001b[34m96%|ââââââââââ| 406/425 [00:17<00:00, 24.22it/s]\u001b[0m\n",
      "\u001b[34m96%|ââââââââââ| 409/425 [00:17<00:00, 23.88it/s]\u001b[0m\n",
      "\u001b[34m97%|ââââââââââ| 412/425 [00:17<00:00, 23.26it/s]\u001b[0m\n",
      "\u001b[34m98%|ââââââââââ| 415/425 [00:17<00:00, 23.00it/s]\u001b[0m\n",
      "\u001b[34m98%|ââââââââââ| 418/425 [00:18<00:00, 23.30it/s]\u001b[0m\n",
      "\u001b[34m99%|ââââââââââ| 421/425 [00:18<00:00, 23.53it/s]\u001b[0m\n",
      "\u001b[34m100%|ââââââââââ| 424/425 [00:18<00:00, 23.94it/s]\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1901] 2023-08-22 14:38:40,698 >> \u001b[0m\n",
      "\u001b[34mTraining completed. Do not forget to share your model on huggingface.co/models =)\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1901] 2023-08-22 14:38:40,698 >> \u001b[0m\n",
      "\u001b[34mTraining completed. Do not forget to share your model on huggingface.co/models =)\u001b[0m\n",
      "\u001b[34m100%|ââââââââââ| 425/425 [00:18<00:00, 23.94it/s]\u001b[0m\n",
      "\u001b[34m{'train_runtime': 18.3533, 'train_samples_per_second': 184.926, 'train_steps_per_second': 23.157, 'train_loss': 0.18624504538143383, 'epoch': 1.0}\u001b[0m\n",
      "\u001b[34m100%|ââââââââââ| 425/425 [00:18<00:00, 23.16it/s]\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2709] 2023-08-22 14:38:40,700 >> Saving model checkpoint to /opt/ml/model\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2709] 2023-08-22 14:38:40,700 >> Saving model checkpoint to /opt/ml/model\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:453] 2023-08-22 14:38:40,701 >> Configuration saved in /opt/ml/model/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:453] 2023-08-22 14:38:40,701 >> Configuration saved in /opt/ml/model/config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1704] 2023-08-22 14:38:41,123 >> Model weights saved in /opt/ml/model/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1704] 2023-08-22 14:38:41,123 >> Model weights saved in /opt/ml/model/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2160] 2023-08-22 14:38:41,123 >> tokenizer config file saved in /opt/ml/model/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2160] 2023-08-22 14:38:41,123 >> tokenizer config file saved in /opt/ml/model/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2167] 2023-08-22 14:38:41,124 >> Special tokens file saved in /opt/ml/model/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2167] 2023-08-22 14:38:41,124 >> Special tokens file saved in /opt/ml/model/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m***** train metrics *****\u001b[0m\n",
      "\u001b[34mepoch                    =        1.0\n",
      "  train_loss               =     0.1862\n",
      "  train_runtime            = 0:00:18.35\n",
      "  train_samples            =       3394\n",
      "  train_samples_per_second =    184.926\n",
      "  train_steps_per_second   =     23.157\u001b[0m\n",
      "\u001b[34m08/22/2023 14:38:41 - INFO - __main__ - *** Evaluate ***\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:710] 2023-08-22 14:38:41,174 >> The following columns in the evaluation set don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: ner_tags, tokens, id. If ner_tags, tokens, id are not expected by `DistilBertForTokenClassification.forward`,  you can safely ignore this message.\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:710] 2023-08-22 14:38:41,174 >> The following columns in the evaluation set don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: ner_tags, tokens, id. If ner_tags, tokens, id are not expected by `DistilBertForTokenClassification.forward`,  you can safely ignore this message.\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2964] 2023-08-22 14:38:41,176 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2964] 2023-08-22 14:38:41,176 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2966] 2023-08-22 14:38:41,176 >>   Num examples = 1009\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2969] 2023-08-22 14:38:41,176 >>   Batch size = 8\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2966] 2023-08-22 14:38:41,176 >>   Num examples = 1009\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2969] 2023-08-22 14:38:41,176 >>   Batch size = 8\u001b[0m\n",
      "\u001b[34m0%|          | 0/127 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m9%|â         | 11/127 [00:00<00:01, 107.07it/s]\u001b[0m\n",
      "\u001b[34m17%|ââ        | 22/127 [00:00<00:01, 104.18it/s]\u001b[0m\n",
      "\u001b[34m26%|âââ       | 33/127 [00:00<00:00, 102.61it/s]\u001b[0m\n",
      "\u001b[34m35%|ââââ      | 44/127 [00:00<00:00, 101.89it/s]\u001b[0m\n",
      "\u001b[34m43%|âââââ     | 55/127 [00:00<00:00, 101.73it/s]\u001b[0m\n",
      "\u001b[34m52%|ââââââ    | 66/127 [00:00<00:00, 101.99it/s]\u001b[0m\n",
      "\u001b[34m61%|ââââââ    | 77/127 [00:00<00:00, 102.02it/s]\u001b[0m\n",
      "\u001b[34m69%|âââââââ   | 88/127 [00:00<00:00, 101.38it/s]\u001b[0m\n",
      "\u001b[34m78%|ââââââââ  | 99/127 [00:00<00:00, 100.57it/s]\u001b[0m\n",
      "\u001b[34m87%|âââââââââ | 110/127 [00:01<00:00, 100.77it/s]\u001b[0m\n",
      "\u001b[34m95%|ââââââââââ| 121/127 [00:01<00:00, 100.50it/s]\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.9/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\u001b[0m\n",
      "\u001b[34m100%|ââââââââââ| 127/127 [00:01<00:00, 79.14it/s]\u001b[0m\n",
      "\u001b[34m***** eval metrics *****\u001b[0m\n",
      "\u001b[34mepoch                   =        1.0\n",
      "  eval_accuracy           =     0.9494\n",
      "  eval_f1                 =     0.5572\n",
      "  eval_loss               =     0.2312\n",
      "  eval_precision          =     0.6998\n",
      "  eval_recall             =     0.4629\n",
      "  eval_runtime            = 0:00:01.61\n",
      "  eval_samples            =       1009\n",
      "  eval_samples_per_second =    624.202\n",
      "  eval_steps_per_second   =     78.567\u001b[0m\n",
      "\u001b[34m2023-08-22 14:38:43,662 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2023-08-22 14:38:43,662 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2023-08-22 14:38:43,663 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# access the logs of the training job\n",
    "huggingface_estimator.sagemaker_session.logs_for_job(huggingface_estimator.latest_training_job.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attach to old training job to an estimator \n",
    "\n",
    "In Sagemaker you can attach an old training job to an estimator to continue training, get results etc.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.estimator import Estimator\n",
    "\n",
    "# job which is going to be attached to the estimator\n",
    "old_training_job_name=''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ParamValidationError",
     "evalue": "Parameter validation failed:\nInvalid length for parameter TrainingJobName, value: 0, valid min length: 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mParamValidationError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-0171a61a49a5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# attach old training job\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mhuggingface_estimator_loaded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEstimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mold_training_job_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# get model output s3 from training job\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mattach\u001b[0;34m(cls, training_job_name, sagemaker_session, model_channel_name)\u001b[0m\n\u001b[1;32m   1430\u001b[0m             \u001b[0mtraining\u001b[0m \u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1431\u001b[0m         \"\"\"\n\u001b[0;32m-> 1432\u001b[0;31m         return cls._attach(\n\u001b[0m\u001b[1;32m   1433\u001b[0m             \u001b[0mtraining_job_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_job_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1434\u001b[0m             \u001b[0msagemaker_session\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36m_attach\u001b[0;34m(cls, training_job_name, sagemaker_session, model_channel_name, additional_kwargs)\u001b[0m\n\u001b[1;32m   1450\u001b[0m         \u001b[0msagemaker_session\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msagemaker_session\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1451\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1452\u001b[0;31m         job_details = sagemaker_session.sagemaker_client.describe_training_job(\n\u001b[0m\u001b[1;32m   1453\u001b[0m             \u001b[0mTrainingJobName\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_job_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1454\u001b[0m         )\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    533\u001b[0m                 )\n\u001b[1;32m    534\u001b[0m             \u001b[0;31m# The \"self\" in this scope is referring to the BaseClient.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 535\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_api_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperation_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    536\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m         \u001b[0m_api_call\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy_operation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    934\u001b[0m             \u001b[0moperation_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapi_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m         )\n\u001b[0;32m--> 936\u001b[0;31m         request_dict = self._convert_to_request_dict(\n\u001b[0m\u001b[1;32m    937\u001b[0m             \u001b[0mapi_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mapi_params\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    938\u001b[0m             \u001b[0moperation_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moperation_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_convert_to_request_dict\u001b[0;34m(self, api_params, operation_model, endpoint_url, context, headers, set_user_agent_header)\u001b[0m\n\u001b[1;32m   1005\u001b[0m         \u001b[0mset_user_agent_header\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m     ):\n\u001b[0;32m-> 1007\u001b[0;31m         request_dict = self._serializer.serialize_to_request(\n\u001b[0m\u001b[1;32m   1008\u001b[0m             \u001b[0mapi_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperation_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1009\u001b[0m         )\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/botocore/validate.py\u001b[0m in \u001b[0;36mserialize_to_request\u001b[0;34m(self, parameters, operation_model)\u001b[0m\n\u001b[1;32m    379\u001b[0m             )\n\u001b[1;32m    380\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mreport\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_errors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mParamValidationError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreport\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreport\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    382\u001b[0m         return self._serializer.serialize_to_request(\n\u001b[1;32m    383\u001b[0m             \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperation_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mParamValidationError\u001b[0m: Parameter validation failed:\nInvalid length for parameter TrainingJobName, value: 0, valid min length: 1"
     ]
    }
   ],
   "source": [
    "# attach old training job\n",
    "huggingface_estimator_loaded = Estimator.attach(old_training_job_name)\n",
    "\n",
    "\n",
    "# get model output s3 from training job\n",
    "huggingface_estimator_loaded.model_data"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "interpreter": {
   "hash": "c281c456f1b8161c8906f4af2c08ed2c40c50136979eaae69688b01f70e9f4a9"
  },
  "kernelspec": {
   "display_name": "Python 3 (PyTorch 1.13 Python 3.9 CPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/pytorch-1.13-cpu-py39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
